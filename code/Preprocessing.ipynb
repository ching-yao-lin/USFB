{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laden-paradise",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Main Goal: To downsize the input files, by casting data types or getting rid of data that are completely useless (for analysis)\n",
    "Preprocessed data will be stored under the `temp` directory as recommended by the given README.<br>\n",
    "Even null values are kept. Only columns that are totally useless will be thrown away.\n",
    "\n",
    "<a id=\"navigation\"></a>\n",
    "\n",
    "## Navigation\n",
    "* #### Comment Data\n",
    "    * [Pickle the Commment Data (for speed)](#pickle-comment)\n",
    "    * [Take a deeper look into fasttext (language detection model)](#fasttext)\n",
    "    * [Preprocessing Comment Data](#preprocessing-comment-data)\n",
    "    * [Dealing with Contractions](#dealing-with-contractions)\n",
    "    * [Add spacyTextblob Sentiment Polarity and Subjectivity](#add-spacytextblob)\n",
    "* #### Post Data\n",
    "    * [preprocessing post data](#post-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-springer",
   "metadata": {},
   "source": [
    "#### Suggested workflow from given README\n",
    "* Use relative paths in the code\n",
    "* Read data from _input_\n",
    "* Export generated tables or figures to _output_\n",
    "* Read/Write other temporary files from _temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "front-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home3/r09725056/Desktop/analysis-ChingYaoL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pickle\n",
    "os.chdir('/home3/r09725056/Desktop/analysis-ChingYaoL')\n",
    "print(os.getcwd())\n",
    "# Use relative paths in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input code output temp .git README.md README_about_USFB_Data.pdf "
     ]
    }
   ],
   "source": [
    "for file in os.listdir():\n",
    "    print(file, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "urban-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # 讓 DataFrame Output可以重複疊起來\n",
    "pd.set_option('display.max_columns', None) # show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remarkable-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import fasttext\n",
    "import contractions # resolving contractions and slangs, e.g. \"yall're happy now\" --> \"you all are happy now\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "textile-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prepared-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "# demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-cement",
   "metadata": {},
   "source": [
    "* [PyPI for fasttext](https://pypi.org/project/fasttext/)\n",
    "* [medium for fasttext](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)\n",
    "* [GitHub for contractions](https://github.com/kootenpv/contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-diploma",
   "metadata": {},
   "source": [
    "![tree.png](tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-institute",
   "metadata": {},
   "source": [
    "## 2016 US Presidential Election: Nov 8, 2016 (Tue)\n",
    "\n",
    "#### **Page**\n",
    "* page_info\n",
    "* politician_info\n",
    "* page_politician_info \n",
    "\n",
    "#### **Post**\n",
    "* top_1000_pages\n",
    "* politician_pages\n",
    "\n",
    "#### **Reaction**\n",
    "**1. Reactions on 1000-page**<br>\n",
    "A. Every-20-minutes (2016-09-29 ~ 2016-11-21)<br>\n",
    "* LIKE, LOVE, HAHA, WOW, SAD, ANGRY, THANKFUL\n",
    "\n",
    "B. LIKE by US political users (2015-01-01 ~ 2016-11-30)\n",
    "* LIKE_on_1000_page\n",
    "\n",
    "**2. Reactions on politicians**<br>\n",
    "* LIKE_on_politicians\n",
    "\n",
    "#### **Comment**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-patch",
   "metadata": {},
   "source": [
    "# Comment\n",
    "[[Back to Navigation]](#navigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-classroom",
   "metadata": {},
   "source": [
    "### Techniques Adopted for Cleaning Comment Texts\n",
    "\n",
    "#### Minimum Preprocessing\n",
    "1. Drop completely duplicate rows \n",
    "2. Add language labels (using Language Detection Model from fasttext)\n",
    " * the reason I use fasttext: [Benchmarking Language Detection for NLP](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c)\n",
    " * [PyPI for fasttext](https://pypi.org/project/fasttext/)\n",
    " * [medium on fasttext](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)\n",
    " * Meanwhile, Summarize the number of English, non-English, and empty comments, and store the summary in a global counter, `en_counter_comment`\n",
    "3. Filter out non-English or empty comments; then, drop the language column\n",
    "\n",
    "#### Further Preprocessing\n",
    "4. Convert all words to lowercase\n",
    "5. Expand contractions and slangs (eg. yall're cool -> you all are cool): [GitHub for contractions](https://github.com/kootenpv/contractions)\n",
    "6. Remove html `<br>` tags, punctuations, links, newlines, tabs, and shrink consecutive spaces to one\n",
    "7. Remove emojis\n",
    "8. Remove stopwords (using nltk)\n",
    "9. Lemmatize texts (using spacy)\n",
    "10. Pickle (Serialize) the result\n",
    "\n",
    "<a id=\"pickle-comment\"></a>\n",
    "##### Since it always takes a while to read the data, I decided to pickle the comment data first for easier and quicker access  in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_comments(remove_stop=True, lemmatize=True) -> None:\n",
    "    \"Simple Preprocessing: Delete duplicate rows. Then, pickle the comment data.\"\n",
    "    # Set source and destination path\n",
    "    src_path = r'input/comment/2015-01-01-to-2016-11-30'\n",
    "    dest_path = r'temp/comment/original'\n",
    "    # Loop for each file in src_path\n",
    "    for i, f_name in enumerate(sorted(os.listdir(src_path))):\n",
    "        # Set source and destination files\n",
    "        src_file = os.path.join(src_path, f_name)\n",
    "        dest_file = os.path.join(dest_path, f'comment_{i}.pkl')\n",
    "        # Read in Data\n",
    "        usecols = ['comment_message', 'post_id', 'comment_created_time']\n",
    "        temp = pd.read_csv(src_file, parse_dates=['comment_created_time'], usecols=usecols)\n",
    "        # Simple Preprocessing\n",
    "        print(f'{f_name}: {temp.duplicated().sum()} completely duplicate rows')\n",
    "        temp.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        # Pickling\n",
    "        temp.to_pickle(dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-adolescent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pickle_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-ensemble",
   "metadata": {},
   "source": [
    "<a id=\"fasttext\"></a>\n",
    "\n",
    "**Now, before actually preprocessing the data, I would like to <font color=\"blue\">take a deeper look into fasttext.</font><br>**\n",
    "**This helps me to gain some insight into the fasttext model itself, and figure out some ways to <font color=\"blue\">\"save\" some data that shouldn't be filtered out.</font><br>**\n",
    "**A total of 48128 data was saved in the end.**\n",
    "\n",
    "**For your convenience, you may [skip to the section: Preprocessing Comment Data](#preprocessing-comment-data)**, since the the following section is a bit long and may potentially break the logic flow of your reading. What it basically does is simply trying to further optimize the preprocessing process (by getting more English comments). You may skip it without worrying not being able to understand the rest of the notebook. Thanks!\n",
    "\n",
    "## Take a deeper look into Fasttext\n",
    "[[Back to Navigation]](#navigation)\n",
    "\n",
    "1. Why are there null values (in the prediction)?\n",
    "2. Lowercasing some data for improving precision\n",
    "3. Check if Punctuation removal may improve precision\n",
    "4. Check if Punctuation removal for only  uppercase data may improve precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "macro-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the commment data\n",
    "comment = pd.read_pickle(r'temp/comment/original/comment_0.pkl')\n",
    "comment['language'] = comment['comment_message'].apply(predict_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-encounter",
   "metadata": {},
   "source": [
    "#### 1. Why are there null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "parental-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_created_time</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>205344452828349_1214133445282773</td>\n",
       "      <td>2015-03-27 02:48:43+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Linette Aleman</td>\n",
       "      <td>165583971161_10153322962686162</td>\n",
       "      <td>2015-04-19 23:38:13+00:00</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ahora van a estar todo el mes hablando de el c...</td>\n",
       "      <td>174725429795_10154116006284796</td>\n",
       "      <td>2016-01-09 04:19:47+00:00</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bravo</td>\n",
       "      <td>125342830842328_918716924838244</td>\n",
       "      <td>2015-05-11 19:19:14+00:00</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MAKE HILLARY AN INMATE AGAIN</td>\n",
       "      <td>39442131319_10154774303631320</td>\n",
       "      <td>2016-10-28 19:50:04+00:00</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862948</th>\n",
       "      <td>Good r u.</td>\n",
       "      <td>169204449790211_916776711699644</td>\n",
       "      <td>2015-04-25 23:24:08+00:00</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862976</th>\n",
       "      <td>NaN</td>\n",
       "      <td>127559550648374_917129971691324</td>\n",
       "      <td>2015-10-24 03:59:37+00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862986</th>\n",
       "      <td>Estas q,opinan aquí\\nQ,nathalia va hacer coron...</td>\n",
       "      <td>259955926518_10153512283491519</td>\n",
       "      <td>2016-03-16 03:50:38+00:00</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863006</th>\n",
       "      <td>Martin Baliola</td>\n",
       "      <td>7331091005_10154284725951006</td>\n",
       "      <td>2016-08-25 13:31:37+00:00</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863007</th>\n",
       "      <td>Kylla ���� ����</td>\n",
       "      <td>205344452828349_1445705432125572</td>\n",
       "      <td>2016-01-03 06:49:32+00:00</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362667 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message  \\\n",
       "11                                                     NaN   \n",
       "15                                          Linette Aleman   \n",
       "17       Ahora van a estar todo el mes hablando de el c...   \n",
       "42                                                   bravo   \n",
       "46                            MAKE HILLARY AN INMATE AGAIN   \n",
       "...                                                    ...   \n",
       "2862948                                          Good r u.   \n",
       "2862976                                                NaN   \n",
       "2862986  Estas q,opinan aquí\\nQ,nathalia va hacer coron...   \n",
       "2863006                                     Martin Baliola   \n",
       "2863007                                    Kylla ���� ����   \n",
       "\n",
       "                                  post_id      comment_created_time language  \n",
       "11       205344452828349_1214133445282773 2015-03-27 02:48:43+00:00      NaN  \n",
       "15         165583971161_10153322962686162 2015-04-19 23:38:13+00:00       es  \n",
       "17         174725429795_10154116006284796 2016-01-09 04:19:47+00:00       es  \n",
       "42        125342830842328_918716924838244 2015-05-11 19:19:14+00:00       it  \n",
       "46          39442131319_10154774303631320 2016-10-28 19:50:04+00:00       ja  \n",
       "...                                   ...                       ...      ...  \n",
       "2862948   169204449790211_916776711699644 2015-04-25 23:24:08+00:00       de  \n",
       "2862976   127559550648374_917129971691324 2015-10-24 03:59:37+00:00      NaN  \n",
       "2862986    259955926518_10153512283491519 2016-03-16 03:50:38+00:00       es  \n",
       "2863006      7331091005_10154284725951006 2016-08-25 13:31:37+00:00       id  \n",
       "2863007  205344452828349_1445705432125572 2016-01-03 06:49:32+00:00       ru  \n",
       "\n",
       "[362667 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment[comment['language'] != 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "finished-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_message         57872\n",
       "post_id                     0\n",
       "comment_created_time        0\n",
       "language                57872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "steady-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_message    57872\n",
       "language           57872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If language is null, then comment_message is null\n",
    "comment.loc[comment['language'].isna(), ['comment_message', 'language']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dependent-robinson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_message    57872\n",
       "language           57872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If comment_message is null, then language is null\n",
    "comment.loc[comment['comment_message'].isna(), ['comment_message', 'language']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-ending",
   "metadata": {},
   "source": [
    "##### All null values/labels result from null comments. Thus, we can safely drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "featured-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can safely drop all\n",
    "comment.dropna(subset=['language'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acceptable-estonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805137, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-durham",
   "metadata": {},
   "source": [
    "#### 2. Lowercasing for improving precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "resident-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment['language'].unique()\n",
    "# comment.loc[comment['language'] == 'es', 'comment_message'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "apparent-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment['comment_message_lower'] = comment['comment_message'].str.lower()\n",
    "comment['lower_language'] = comment['comment_message_lower'].apply(predict_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "entire-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filters\n",
    "lang_diff = comment['language'] != comment['lower_language']\n",
    "cmt_en = comment['language'] == 'en'\n",
    "cmt_lower_en = comment['lower_language'] == 'en'\n",
    "# Define columns of interest\n",
    "cmt_lang_cols = ['comment_message', 'language', 'comment_message_lower', 'lower_language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "angry-geography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_lower</th>\n",
       "      <th>lower_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54218</th>\n",
       "      <td>Sore loser.</td>\n",
       "      <td>en</td>\n",
       "      <td>sore loser.</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658600</th>\n",
       "      <td>Islam Tyler Bradley Albertas</td>\n",
       "      <td>en</td>\n",
       "      <td>islam tyler bradley albertas</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477153</th>\n",
       "      <td>Patty NG</td>\n",
       "      <td>en</td>\n",
       "      <td>patty ng</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891299</th>\n",
       "      <td>Kyle Skrivanek</td>\n",
       "      <td>en</td>\n",
       "      <td>kyle skrivanek</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079815</th>\n",
       "      <td>Alberto Alvarez</td>\n",
       "      <td>en</td>\n",
       "      <td>alberto alvarez</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463743</th>\n",
       "      <td>Kristi Hartman</td>\n",
       "      <td>en</td>\n",
       "      <td>kristi hartman</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485387</th>\n",
       "      <td>Tarra Cheshire ��</td>\n",
       "      <td>en</td>\n",
       "      <td>tarra cheshire ��</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642490</th>\n",
       "      <td>Oh Jr.</td>\n",
       "      <td>en</td>\n",
       "      <td>oh jr.</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504496</th>\n",
       "      <td>Donovan Casares ��</td>\n",
       "      <td>en</td>\n",
       "      <td>donovan casares ��</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296785</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668820</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266646</th>\n",
       "      <td>Brad Tronina</td>\n",
       "      <td>en</td>\n",
       "      <td>brad tronina</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954944</th>\n",
       "      <td>Shazy Goni</td>\n",
       "      <td>en</td>\n",
       "      <td>shazy goni</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833065</th>\n",
       "      <td>NEVER!!!!!!!!! Dont give up cena plzzz</td>\n",
       "      <td>en</td>\n",
       "      <td>never!!!!!!!!! dont give up cena plzzz</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162068</th>\n",
       "      <td>Aaron L. Johnson.</td>\n",
       "      <td>en</td>\n",
       "      <td>aaron l. johnson.</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425485</th>\n",
       "      <td>OH HELL YES!</td>\n",
       "      <td>en</td>\n",
       "      <td>oh hell yes!</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627480</th>\n",
       "      <td>GOD</td>\n",
       "      <td>en</td>\n",
       "      <td>god</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418781</th>\n",
       "      <td>Deirdre</td>\n",
       "      <td>en</td>\n",
       "      <td>deirdre</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242288</th>\n",
       "      <td>R I P ��</td>\n",
       "      <td>en</td>\n",
       "      <td>r i p ��</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648618</th>\n",
       "      <td>Orlando</td>\n",
       "      <td>en</td>\n",
       "      <td>orlando</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872671</th>\n",
       "      <td>Erika Correri</td>\n",
       "      <td>en</td>\n",
       "      <td>erika correri</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743059</th>\n",
       "      <td>Kristen 'kMay' May</td>\n",
       "      <td>en</td>\n",
       "      <td>kristen 'kmay' may</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394954</th>\n",
       "      <td>Ian Long</td>\n",
       "      <td>en</td>\n",
       "      <td>ian long</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348427</th>\n",
       "      <td>Sweet angel.</td>\n",
       "      <td>en</td>\n",
       "      <td>sweet angel.</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106130</th>\n",
       "      <td>Nancy La Ernie ��</td>\n",
       "      <td>en</td>\n",
       "      <td>nancy la ernie ��</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249508</th>\n",
       "      <td>Charles Derrien  hahaha</td>\n",
       "      <td>en</td>\n",
       "      <td>charles derrien  hahaha</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569487</th>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>true</td>\n",
       "      <td>fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87684</th>\n",
       "      <td>Obama thugs</td>\n",
       "      <td>en</td>\n",
       "      <td>obama thugs</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525941</th>\n",
       "      <td>Teeih Harris</td>\n",
       "      <td>en</td>\n",
       "      <td>teeih harris</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648778</th>\n",
       "      <td>Juliano Patricia</td>\n",
       "      <td>en</td>\n",
       "      <td>juliano patricia</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570234</th>\n",
       "      <td>ASS HOLE</td>\n",
       "      <td>en</td>\n",
       "      <td>ass hole</td>\n",
       "      <td>lb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224261</th>\n",
       "      <td>Armando</td>\n",
       "      <td>en</td>\n",
       "      <td>armando</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452768</th>\n",
       "      <td>Michelle Iris</td>\n",
       "      <td>en</td>\n",
       "      <td>michelle iris</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688552</th>\n",
       "      <td>Jake Lloyd Peña dem plot holes tho hahaha</td>\n",
       "      <td>en</td>\n",
       "      <td>jake lloyd peña dem plot holes tho hahaha</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562128</th>\n",
       "      <td>Sad</td>\n",
       "      <td>en</td>\n",
       "      <td>sad</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754201</th>\n",
       "      <td>Never.</td>\n",
       "      <td>en</td>\n",
       "      <td>never.</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878361</th>\n",
       "      <td>Monica Paolo</td>\n",
       "      <td>en</td>\n",
       "      <td>monica paolo</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123907</th>\n",
       "      <td>Hahahah gooooo france.. marche marche going ou...</td>\n",
       "      <td>en</td>\n",
       "      <td>hahahah gooooo france.. marche marche going ou...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105570</th>\n",
       "      <td>Gisselle Mencia Labrenz .... need this</td>\n",
       "      <td>en</td>\n",
       "      <td>gisselle mencia labrenz .... need this</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837520</th>\n",
       "      <td>Natalie Robson</td>\n",
       "      <td>en</td>\n",
       "      <td>natalie robson</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86729</th>\n",
       "      <td>The FBI? Buuahahahahahah.</td>\n",
       "      <td>en</td>\n",
       "      <td>the fbi? buuahahahahahah.</td>\n",
       "      <td>min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210913</th>\n",
       "      <td>Liam Sullivan Alex Cusano</td>\n",
       "      <td>en</td>\n",
       "      <td>liam sullivan alex cusano</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333465</th>\n",
       "      <td>Helen McMelon</td>\n",
       "      <td>en</td>\n",
       "      <td>helen mcmelon</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161469</th>\n",
       "      <td>Amen ��</td>\n",
       "      <td>en</td>\n",
       "      <td>amen ��</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338585</th>\n",
       "      <td>Sam Simon xD</td>\n",
       "      <td>en</td>\n",
       "      <td>sam simon xd</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575997</th>\n",
       "      <td>Lauren Levine...</td>\n",
       "      <td>en</td>\n",
       "      <td>lauren levine...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400333</th>\n",
       "      <td>Ricky Puopolo</td>\n",
       "      <td>en</td>\n",
       "      <td>ricky puopolo</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600192</th>\n",
       "      <td>Hurry the Hell up and get the Hell out of the ...</td>\n",
       "      <td>en</td>\n",
       "      <td>hurry the hell up and get the hell out of the ...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763649</th>\n",
       "      <td>Maria Flores</td>\n",
       "      <td>en</td>\n",
       "      <td>maria flores</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232398</th>\n",
       "      <td>Oye #latinosforbernie</td>\n",
       "      <td>en</td>\n",
       "      <td>oye #latinosforbernie</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "54218                                          Sore loser.       en   \n",
       "658600                        Islam Tyler Bradley Albertas       en   \n",
       "2477153                                           Patty NG       en   \n",
       "1891299                                     Kyle Skrivanek       en   \n",
       "2079815                                    Alberto Alvarez       en   \n",
       "1463743                                     Kristi Hartman       en   \n",
       "1485387                                  Tarra Cheshire ��       en   \n",
       "2642490                                             Oh Jr.       en   \n",
       "504496                                  Donovan Casares ��       en   \n",
       "2296785                                               Amen       en   \n",
       "668820                                                Amen       en   \n",
       "2266646                                       Brad Tronina       en   \n",
       "954944                                          Shazy Goni       en   \n",
       "833065              NEVER!!!!!!!!! Dont give up cena plzzz       en   \n",
       "2162068                                  Aaron L. Johnson.       en   \n",
       "425485                                        OH HELL YES!       en   \n",
       "2627480                                                GOD       en   \n",
       "418781                                             Deirdre       en   \n",
       "242288                                            R I P ��       en   \n",
       "1648618                                            Orlando       en   \n",
       "1872671                                      Erika Correri       en   \n",
       "743059                                  Kristen 'kMay' May       en   \n",
       "394954                                            Ian Long       en   \n",
       "2348427                                       Sweet angel.       en   \n",
       "2106130                                  Nancy La Ernie ��       en   \n",
       "249508                             Charles Derrien  hahaha       en   \n",
       "569487                                                True       en   \n",
       "87684                                          Obama thugs       en   \n",
       "525941                                        Teeih Harris       en   \n",
       "2648778                                   Juliano Patricia       en   \n",
       "1570234                                           ASS HOLE       en   \n",
       "224261                                             Armando       en   \n",
       "452768                                       Michelle Iris       en   \n",
       "688552           Jake Lloyd Peña dem plot holes tho hahaha       en   \n",
       "1562128                                                Sad       en   \n",
       "754201                                              Never.       en   \n",
       "878361                                        Monica Paolo       en   \n",
       "2123907  Hahahah gooooo france.. marche marche going ou...       en   \n",
       "105570              Gisselle Mencia Labrenz .... need this       en   \n",
       "837520                                      Natalie Robson       en   \n",
       "86729                            The FBI? Buuahahahahahah.       en   \n",
       "2210913                          Liam Sullivan Alex Cusano       en   \n",
       "2333465                                      Helen McMelon       en   \n",
       "161469                                             Amen ��       en   \n",
       "1338585                                       Sam Simon xD       en   \n",
       "575997                                    Lauren Levine...       en   \n",
       "1400333                                      Ricky Puopolo       en   \n",
       "600192   Hurry the Hell up and get the Hell out of the ...       en   \n",
       "1763649                                       Maria Flores       en   \n",
       "2232398                              Oye #latinosforbernie       en   \n",
       "\n",
       "                                     comment_message_lower lower_language  \n",
       "54218                                          sore loser.             it  \n",
       "658600                        islam tyler bradley albertas             es  \n",
       "2477153                                           patty ng             tl  \n",
       "1891299                                     kyle skrivanek             lt  \n",
       "2079815                                    alberto alvarez             it  \n",
       "1463743                                     kristi hartman             fi  \n",
       "1485387                                  tarra cheshire ��             pt  \n",
       "2642490                                             oh jr.             nl  \n",
       "504496                                  donovan casares ��             es  \n",
       "2296785                                               amen             ca  \n",
       "668820                                                amen             ca  \n",
       "2266646                                       brad tronina             fr  \n",
       "954944                                          shazy goni             it  \n",
       "833065              never!!!!!!!!! dont give up cena plzzz             fr  \n",
       "2162068                                  aaron l. johnson.            war  \n",
       "425485                                        oh hell yes!             de  \n",
       "2627480                                                god             sv  \n",
       "418781                                             deirdre             pt  \n",
       "242288                                            r i p ��             sv  \n",
       "1648618                                            orlando             pt  \n",
       "1872671                                      erika correri             pt  \n",
       "743059                                  kristen 'kmay' may             de  \n",
       "394954                                            ian long             pt  \n",
       "2348427                                       sweet angel.             no  \n",
       "2106130                                  nancy la ernie ��             fr  \n",
       "249508                             charles derrien  hahaha             de  \n",
       "569487                                                true             fa  \n",
       "87684                                          obama thugs             ca  \n",
       "525941                                        teeih harris             fi  \n",
       "2648778                                   juliano patricia             es  \n",
       "1570234                                           ass hole             lb  \n",
       "224261                                             armando             it  \n",
       "452768                                       michelle iris             eo  \n",
       "688552           jake lloyd peña dem plot holes tho hahaha             de  \n",
       "1562128                                                sad             bs  \n",
       "754201                                              never.             zh  \n",
       "878361                                        monica paolo             it  \n",
       "2123907  hahahah gooooo france.. marche marche going ou...             fr  \n",
       "105570              gisselle mencia labrenz .... need this             es  \n",
       "837520                                      natalie robson             fr  \n",
       "86729                            the fbi? buuahahahahahah.            min  \n",
       "2210913                          liam sullivan alex cusano             it  \n",
       "2333465                                      helen mcmelon             fr  \n",
       "161469                                             amen ��             ca  \n",
       "1338585                                       sam simon xd             fr  \n",
       "575997                                    lauren levine...             fr  \n",
       "1400333                                      ricky puopolo             fi  \n",
       "600192   hurry the hell up and get the hell out of the ...             ru  \n",
       "1763649                                       maria flores             pt  \n",
       "2232398                              oye #latinosforbernie             es  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of lang_diff...\n",
    "# Most of those predicted English are, in fact, really English!\n",
    "# But were predicted otherwise after being transformed to lowercase. (Red Arrow)\n",
    "comment.loc[lang_diff & cmt_en, cmt_lang_cols].sample(50, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "detailed-pearl",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_lower</th>\n",
       "      <th>lower_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764553</th>\n",
       "      <td>Yanitza Monk Presella Tomsjansen ooo hell to t...</td>\n",
       "      <td>nl</td>\n",
       "      <td>yanitza monk presella tomsjansen ooo hell to t...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970903</th>\n",
       "      <td>Well......bye</td>\n",
       "      <td>pt</td>\n",
       "      <td>well......bye</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223100</th>\n",
       "      <td>C</td>\n",
       "      <td>it</td>\n",
       "      <td>c</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862500</th>\n",
       "      <td>Deadspin: \"Remember Leney Kukua? Guys? Anyone!?\"</td>\n",
       "      <td>de</td>\n",
       "      <td>deadspin: \"remember leney kukua? guys? anyone!?\"</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811534</th>\n",
       "      <td>TRUMP/PENCE����</td>\n",
       "      <td>ru</td>\n",
       "      <td>trump/pence����</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992414</th>\n",
       "      <td>Thats cute ^°^ ����</td>\n",
       "      <td>ru</td>\n",
       "      <td>thats cute ^°^ ����</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594121</th>\n",
       "      <td>Happy Birthday Pres.Bush</td>\n",
       "      <td>es</td>\n",
       "      <td>happy birthday pres.bush</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440355</th>\n",
       "      <td>POOR ICE ICE BABY!!</td>\n",
       "      <td>ja</td>\n",
       "      <td>poor ice ice baby!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274308</th>\n",
       "      <td>Scumbag!</td>\n",
       "      <td>de</td>\n",
       "      <td>scumbag!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521840</th>\n",
       "      <td>Aww</td>\n",
       "      <td>jbo</td>\n",
       "      <td>aww</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107191</th>\n",
       "      <td>HURRAY!</td>\n",
       "      <td>id</td>\n",
       "      <td>hurray!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284991</th>\n",
       "      <td>I AGREE  BOB</td>\n",
       "      <td>es</td>\n",
       "      <td>i agree  bob</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996274</th>\n",
       "      <td>Traitor.</td>\n",
       "      <td>es</td>\n",
       "      <td>traitor.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257628</th>\n",
       "      <td>Upset havens!</td>\n",
       "      <td>sv</td>\n",
       "      <td>upset havens!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796929</th>\n",
       "      <td>OH!!  GIVE ME A _____BREAK...</td>\n",
       "      <td>ja</td>\n",
       "      <td>oh!!  give me a _____break...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663511</th>\n",
       "      <td>Aren</td>\n",
       "      <td>nl</td>\n",
       "      <td>aren</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817361</th>\n",
       "      <td>BEAUTIFUL HEAD SHOT</td>\n",
       "      <td>ja</td>\n",
       "      <td>beautiful head shot</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130639</th>\n",
       "      <td>Lisa haha</td>\n",
       "      <td>de</td>\n",
       "      <td>lisa haha</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497307</th>\n",
       "      <td>POS</td>\n",
       "      <td>de</td>\n",
       "      <td>pos</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041797</th>\n",
       "      <td>Sara Ashley Castaneda</td>\n",
       "      <td>es</td>\n",
       "      <td>sara ashley castaneda</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377237</th>\n",
       "      <td>YOUR THE WORSE WISH WE COULD TAKE YOUR STATE F...</td>\n",
       "      <td>ja</td>\n",
       "      <td>your the worse wish we could take your state f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436875</th>\n",
       "      <td>TRUMP WILL BE IMPEACHED AS SOON AS HIS ASS HIT...</td>\n",
       "      <td>ja</td>\n",
       "      <td>trump will be impeached as soon as his ass hit...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763205</th>\n",
       "      <td>SEEN!!!!!</td>\n",
       "      <td>da</td>\n",
       "      <td>seen!!!!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785488</th>\n",
       "      <td>EVERY TIME I HEAR THAT VOICE I GET DIZZY !!!</td>\n",
       "      <td>ja</td>\n",
       "      <td>every time i hear that voice i get dizzy !!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264245</th>\n",
       "      <td>Elisabeta Bačíkovic ��</td>\n",
       "      <td>sv</td>\n",
       "      <td>elisabeta bačíkovic ��</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642297</th>\n",
       "      <td>LOCK THEM ALL UP</td>\n",
       "      <td>ja</td>\n",
       "      <td>lock them all up</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781436</th>\n",
       "      <td>ENOUGH ALREADY! THE CREEP IS DELUSIONAL!</td>\n",
       "      <td>pt</td>\n",
       "      <td>enough already! the creep is delusional!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633329</th>\n",
       "      <td>But ya</td>\n",
       "      <td>sw</td>\n",
       "      <td>but ya</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670518</th>\n",
       "      <td>DEFINITELY A FAMILY TRADITION!</td>\n",
       "      <td>ja</td>\n",
       "      <td>definitely a family tradition!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633223</th>\n",
       "      <td>SPEAK-OUT NOW AGAINST THE COMING TYRANNY!</td>\n",
       "      <td>ja</td>\n",
       "      <td>speak-out now against the coming tyranny!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264887</th>\n",
       "      <td>Bye</td>\n",
       "      <td>da</td>\n",
       "      <td>bye</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489795</th>\n",
       "      <td>BRAVO GIRL!</td>\n",
       "      <td>ja</td>\n",
       "      <td>bravo girl!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77080</th>\n",
       "      <td>tWO BAD, BAD WOMEN!!</td>\n",
       "      <td>ja</td>\n",
       "      <td>two bad, bad women!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366976</th>\n",
       "      <td>CHUMPLY</td>\n",
       "      <td>zh</td>\n",
       "      <td>chumply</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173727</th>\n",
       "      <td>Home run!</td>\n",
       "      <td>id</td>\n",
       "      <td>home run!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809565</th>\n",
       "      <td>Bye</td>\n",
       "      <td>da</td>\n",
       "      <td>bye</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622497</th>\n",
       "      <td>Luciana Hammerat Ribeiro</td>\n",
       "      <td>pt</td>\n",
       "      <td>luciana hammerat ribeiro</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682130</th>\n",
       "      <td>POS</td>\n",
       "      <td>de</td>\n",
       "      <td>pos</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065349</th>\n",
       "      <td>Daniel Peralez</td>\n",
       "      <td>fr</td>\n",
       "      <td>daniel peralez</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808190</th>\n",
       "      <td>Up</td>\n",
       "      <td>sv</td>\n",
       "      <td>up</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805861</th>\n",
       "      <td>Thank god</td>\n",
       "      <td>sv</td>\n",
       "      <td>thank god</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709740</th>\n",
       "      <td>PENCE IS A BOSS!!!</td>\n",
       "      <td>ja</td>\n",
       "      <td>pence is a boss!!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129002</th>\n",
       "      <td>Leon Idk</td>\n",
       "      <td>da</td>\n",
       "      <td>leon idk</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841978</th>\n",
       "      <td>NO!!!</td>\n",
       "      <td>ru</td>\n",
       "      <td>no!!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396119</th>\n",
       "      <td>LMAO....YOU WISH OTHER WAY AROUND</td>\n",
       "      <td>ja</td>\n",
       "      <td>lmao....you wish other way around</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779576</th>\n",
       "      <td>#TAYLOR SWIFT!!!!</td>\n",
       "      <td>pl</td>\n",
       "      <td>#taylor swift!!!!</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679621</th>\n",
       "      <td>CNN, ENTERTAINMENT TONIGHT,  SAME SAME...</td>\n",
       "      <td>zh</td>\n",
       "      <td>cnn, entertainment tonight,  same same...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492246</th>\n",
       "      <td>Felix Xilef Nico Randy Manuel Ahlert Max Knebe...</td>\n",
       "      <td>ca</td>\n",
       "      <td>felix xilef nico randy manuel ahlert max knebe...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524643</th>\n",
       "      <td>Dom Thorpe</td>\n",
       "      <td>pt</td>\n",
       "      <td>dom thorpe</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424468</th>\n",
       "      <td>AHAHAHAHAHAHAH Selena Lewis Crosswire</td>\n",
       "      <td>id</td>\n",
       "      <td>ahahahahahahah selena lewis crosswire</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "764553   Yanitza Monk Presella Tomsjansen ooo hell to t...       nl   \n",
       "970903                                       Well......bye       pt   \n",
       "223100                                                   C       it   \n",
       "2862500   Deadspin: \"Remember Leney Kukua? Guys? Anyone!?\"       de   \n",
       "1811534                                    TRUMP/PENCE����       ru   \n",
       "1992414                                Thats cute ^°^ ����       ru   \n",
       "1594121                           Happy Birthday Pres.Bush       es   \n",
       "440355                                 POOR ICE ICE BABY!!       ja   \n",
       "2274308                                           Scumbag!       de   \n",
       "521840                                                 Aww      jbo   \n",
       "107191                                             HURRAY!       id   \n",
       "1284991                                       I AGREE  BOB       es   \n",
       "996274                                            Traitor.       es   \n",
       "2257628                                      Upset havens!       sv   \n",
       "2796929                      OH!!  GIVE ME A _____BREAK...       ja   \n",
       "2663511                                               Aren       nl   \n",
       "1817361                                BEAUTIFUL HEAD SHOT       ja   \n",
       "1130639                                          Lisa haha       de   \n",
       "1497307                                                POS       de   \n",
       "2041797                              Sara Ashley Castaneda       es   \n",
       "1377237  YOUR THE WORSE WISH WE COULD TAKE YOUR STATE F...       ja   \n",
       "2436875  TRUMP WILL BE IMPEACHED AS SOON AS HIS ASS HIT...       ja   \n",
       "2763205                                          SEEN!!!!!       da   \n",
       "2785488       EVERY TIME I HEAR THAT VOICE I GET DIZZY !!!       ja   \n",
       "1264245                             Elisabeta Bačíkovic ��       sv   \n",
       "1642297                                   LOCK THEM ALL UP       ja   \n",
       "2781436           ENOUGH ALREADY! THE CREEP IS DELUSIONAL!       pt   \n",
       "633329                                              But ya       sw   \n",
       "2670518                     DEFINITELY A FAMILY TRADITION!       ja   \n",
       "1633223          SPEAK-OUT NOW AGAINST THE COMING TYRANNY!       ja   \n",
       "2264887                                                Bye       da   \n",
       "2489795                                        BRAVO GIRL!       ja   \n",
       "77080                                 tWO BAD, BAD WOMEN!!       ja   \n",
       "1366976                                            CHUMPLY       zh   \n",
       "173727                                           Home run!       id   \n",
       "1809565                                                Bye       da   \n",
       "2622497                           Luciana Hammerat Ribeiro       pt   \n",
       "1682130                                                POS       de   \n",
       "1065349                                     Daniel Peralez       fr   \n",
       "808190                                                  Up       sv   \n",
       "805861                                           Thank god       sv   \n",
       "709740                                  PENCE IS A BOSS!!!       ja   \n",
       "129002                                            Leon Idk       da   \n",
       "841978                                               NO!!!       ru   \n",
       "396119                   LMAO....YOU WISH OTHER WAY AROUND       ja   \n",
       "1779576                                  #TAYLOR SWIFT!!!!       pl   \n",
       "2679621          CNN, ENTERTAINMENT TONIGHT,  SAME SAME...       zh   \n",
       "492246   Felix Xilef Nico Randy Manuel Ahlert Max Knebe...       ca   \n",
       "2524643                                         Dom Thorpe       pt   \n",
       "424468               AHAHAHAHAHAHAH Selena Lewis Crosswire       id   \n",
       "\n",
       "                                     comment_message_lower lower_language  \n",
       "764553   yanitza monk presella tomsjansen ooo hell to t...             en  \n",
       "970903                                       well......bye             en  \n",
       "223100                                                   c             en  \n",
       "2862500   deadspin: \"remember leney kukua? guys? anyone!?\"             en  \n",
       "1811534                                    trump/pence����             en  \n",
       "1992414                                thats cute ^°^ ����             en  \n",
       "1594121                           happy birthday pres.bush             en  \n",
       "440355                                 poor ice ice baby!!             en  \n",
       "2274308                                           scumbag!             en  \n",
       "521840                                                 aww             en  \n",
       "107191                                             hurray!             en  \n",
       "1284991                                       i agree  bob             en  \n",
       "996274                                            traitor.             en  \n",
       "2257628                                      upset havens!             en  \n",
       "2796929                      oh!!  give me a _____break...             en  \n",
       "2663511                                               aren             en  \n",
       "1817361                                beautiful head shot             en  \n",
       "1130639                                          lisa haha             en  \n",
       "1497307                                                pos             en  \n",
       "2041797                              sara ashley castaneda             en  \n",
       "1377237  your the worse wish we could take your state f...             en  \n",
       "2436875  trump will be impeached as soon as his ass hit...             en  \n",
       "2763205                                          seen!!!!!             en  \n",
       "2785488       every time i hear that voice i get dizzy !!!             en  \n",
       "1264245                             elisabeta bačíkovic ��             en  \n",
       "1642297                                   lock them all up             en  \n",
       "2781436           enough already! the creep is delusional!             en  \n",
       "633329                                              but ya             en  \n",
       "2670518                     definitely a family tradition!             en  \n",
       "1633223          speak-out now against the coming tyranny!             en  \n",
       "2264887                                                bye             en  \n",
       "2489795                                        bravo girl!             en  \n",
       "77080                                 two bad, bad women!!             en  \n",
       "1366976                                            chumply             en  \n",
       "173727                                           home run!             en  \n",
       "1809565                                                bye             en  \n",
       "2622497                           luciana hammerat ribeiro             en  \n",
       "1682130                                                pos             en  \n",
       "1065349                                     daniel peralez             en  \n",
       "808190                                                  up             en  \n",
       "805861                                           thank god             en  \n",
       "709740                                  pence is a boss!!!             en  \n",
       "129002                                            leon idk             en  \n",
       "841978                                               no!!!             en  \n",
       "396119                   lmao....you wish other way around             en  \n",
       "1779576                                  #taylor swift!!!!             en  \n",
       "2679621          cnn, entertainment tonight,  same same...             en  \n",
       "492246   felix xilef nico randy manuel ahlert max knebe...             en  \n",
       "2524643                                         dom thorpe             en  \n",
       "424468               ahahahahahahah selena lewis crosswire             en  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of lang_diff...\n",
    "# Most of those predicted non-English are in reality English!\n",
    "# And were accurately predicted as English after being transformed to lowercase. (Green Arrow)\n",
    "comment.loc[lang_diff & cmt_lower_en, cmt_lang_cols].sample(50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-punch",
   "metadata": {},
   "source": [
    "**Data saved!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "satisfied-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green Arrow: convert those predicted non-English above into English\n",
    "indexer_to_en = comment.loc[lang_diff & cmt_lower_en, cmt_lang_cols].index\n",
    "comment.loc[indexer_to_en, 'language'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "secondary-positive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48128"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexer_to_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "demographic-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the results\n",
    "# comment.loc[lang_diff & cmt_lower_en, cmt_lang_cols].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "corresponding-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = comment.loc[comment['language'] == 'en', ['comment_message', 'post_id', 'comment_created_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "honey-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.to_pickle(r'temp/comment/english_only/comment_0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "exotic-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware that the filters lang_diff, cmt_en, and cmt_lower_en are not updated yet! (boolean series)\n",
    "# comment.loc[lang_diff & cmt_lower_en, cmt_lang_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "welcome-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the filters\n",
    "lang_diff = comment['language'] != comment['lower_language']\n",
    "cmt_en = comment['language'] == 'en'\n",
    "cmt_lower_en = comment['lower_language'] == 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-emission",
   "metadata": {},
   "source": [
    "#### 3. Punctuation Removal (failed attempt)\n",
    "* remove_punc_symbol does not remove emojis\n",
    "* It seems that the effectiveness of fasttext is heavily affected by th presence of punctuations. (At least this is seen when a comment is in full uppercase. We need to check for the general case, which includes both cases of uppercase and lowercase)\n",
    "* NOT favorable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "going-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "comment['comment_message_nopunc'] = comment['comment_message'].apply(remove_punc_symbol)\n",
    "comment['nopunc_language'] = comment['comment_message_nopunc'].apply(predict_language)\n",
    "# Define new filters\n",
    "nopunc_lang_diff = comment['language'] != comment['nopunc_language']\n",
    "cmt_nopunc_en = comment['nopunc_language'] == 'en'\n",
    "# Define columns of interest\n",
    "cmt_lang_cols_nopunc = ['comment_message', 'language', 'comment_message_nopunc', 'nopunc_language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dramatic-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST, we need to check if removing puctuations removes emojis...\n",
    "# Create regex finding emojis\n",
    "emoji_regex = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # includes question mark symbol\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "# Create filters about emoji existence\n",
    "cmt_contains_emoji = comment['comment_message'].apply(lambda x: True if emoji_regex.search(x) else False)\n",
    "cmt_contains_emoji_besides_question_mark = comment['comment_message'].str.replace('�', '').apply(lambda x: True if emoji_regex.search(x) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "liable-ordinance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140495"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "140495"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER: remove_punc_symbol does not removes emojis\n",
    "cmt_contains_emoji.sum()\n",
    "comment['comment_message_nopunc'].apply(lambda x: True if emoji_regex.search(x) else False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "hindu-samba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_nopunc</th>\n",
       "      <th>nopunc_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>868232</th>\n",
       "      <td>yawn...</td>\n",
       "      <td>vi</td>\n",
       "      <td>yawn</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830365</th>\n",
       "      <td>Luv my .9mm  ....</td>\n",
       "      <td>en</td>\n",
       "      <td>Luv my 9mm</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237350</th>\n",
       "      <td>HEE  HEE. HEE</td>\n",
       "      <td>es</td>\n",
       "      <td>HEE HEE HEE</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630209</th>\n",
       "      <td>Lol!</td>\n",
       "      <td>tr</td>\n",
       "      <td>Lol</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Lolol</td>\n",
       "      <td>en</td>\n",
       "      <td>Lolol</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217855</th>\n",
       "      <td>Nah.</td>\n",
       "      <td>id</td>\n",
       "      <td>Nah</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924454</th>\n",
       "      <td>Niklas Hengesbach GTI #pressure</td>\n",
       "      <td>nl</td>\n",
       "      <td>Niklas Hengesbach GTI pressure</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690377</th>\n",
       "      <td>Haters</td>\n",
       "      <td>en</td>\n",
       "      <td>Haters</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793986</th>\n",
       "      <td>Denise FernRos Rosalm hoppas du gjorde det samma!</td>\n",
       "      <td>da</td>\n",
       "      <td>Denise FernRos Rosalm hoppas du gjorde det samma</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791560</th>\n",
       "      <td>Tiffany Luk</td>\n",
       "      <td>en</td>\n",
       "      <td>Tiffany Luk</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811076</th>\n",
       "      <td>https://www.youtube.com/watch?v=FfEiwbWaSbo</td>\n",
       "      <td>en</td>\n",
       "      <td>v FfEiwbWaSbo</td>\n",
       "      <td>cy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577300</th>\n",
       "      <td>https://m.youtube.com/watch?v=lvx4y4J-8aM</td>\n",
       "      <td>en</td>\n",
       "      <td>v lvx4y4J 8aM</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127787</th>\n",
       "      <td>Si.</td>\n",
       "      <td>vec</td>\n",
       "      <td>Si</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168691</th>\n",
       "      <td>That's willpower!</td>\n",
       "      <td>en</td>\n",
       "      <td>That s willpower</td>\n",
       "      <td>ceb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80642</th>\n",
       "      <td>Brianne Fay</td>\n",
       "      <td>en</td>\n",
       "      <td>Brianne Fay</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "868232                                             yawn...       vi   \n",
       "830365                                   Luv my .9mm  ....       en   \n",
       "237350                                       HEE  HEE. HEE       es   \n",
       "630209                                                Lol!       tr   \n",
       "821                                                  Lolol       en   \n",
       "1217855                                               Nah.       id   \n",
       "924454                     Niklas Hengesbach GTI #pressure       nl   \n",
       "2690377                                             Haters       en   \n",
       "2793986  Denise FernRos Rosalm hoppas du gjorde det samma!       da   \n",
       "791560                                         Tiffany Luk       en   \n",
       "1811076        https://www.youtube.com/watch?v=FfEiwbWaSbo       en   \n",
       "1577300          https://m.youtube.com/watch?v=lvx4y4J-8aM       en   \n",
       "127787                                                 Si.      vec   \n",
       "2168691                                  That's willpower!       en   \n",
       "80642                                          Brianne Fay       en   \n",
       "\n",
       "                                    comment_message_nopunc nopunc_language  \n",
       "868232                                               yawn               en  \n",
       "830365                                         Luv my 9mm               zh  \n",
       "237350                                         HEE HEE HEE              de  \n",
       "630209                                                Lol               en  \n",
       "821                                                  Lolol              sv  \n",
       "1217855                                               Nah               en  \n",
       "924454                      Niklas Hengesbach GTI pressure              en  \n",
       "2690377                                             Haters              de  \n",
       "2793986  Denise FernRos Rosalm hoppas du gjorde det samma               sv  \n",
       "791560                                         Tiffany Luk              id  \n",
       "1811076                                      v FfEiwbWaSbo              cy  \n",
       "1577300                                      v lvx4y4J 8aM              es  \n",
       "127787                                                 Si               it  \n",
       "2168691                                  That s willpower              ceb  \n",
       "80642                                          Brianne Fay              fr  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Back to the topic: Punctuation Removal (without removing emojis)\n",
    "# In the cases of nopunc_lang_diff...\n",
    "# Remove punctuations and check out the overall results\n",
    "comment.loc[nopunc_lang_diff, cmt_lang_cols_nopunc].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "romantic-tribe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_nopunc</th>\n",
       "      <th>nopunc_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>645139</th>\n",
       "      <td>No!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>No</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642386</th>\n",
       "      <td>Very intelligent man !!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>Very intelligent man</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211820</th>\n",
       "      <td>Lmao Uzair Piotr Bee</td>\n",
       "      <td>en</td>\n",
       "      <td>Lmao Uzair Piotr Bee</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611778</th>\n",
       "      <td>https://www.youtube.com/watch?v=xI3sA004xlQ</td>\n",
       "      <td>en</td>\n",
       "      <td>v xI3sA004xlQ</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331739</th>\n",
       "      <td>Ashley Larson</td>\n",
       "      <td>en</td>\n",
       "      <td>Ashley Larson</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829581</th>\n",
       "      <td>NO  MOTHER  FUCKER  NEVER DID FOIIOW  YOU  NOT...</td>\n",
       "      <td>en</td>\n",
       "      <td>NO MOTHER FUCKER NEVER DID FOIIOW YOU NOT NEVE...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676961</th>\n",
       "      <td>Fuck no.</td>\n",
       "      <td>en</td>\n",
       "      <td>Fuck no</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312799</th>\n",
       "      <td>\"Bro\"</td>\n",
       "      <td>en</td>\n",
       "      <td>Bro</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528138</th>\n",
       "      <td>Aww.</td>\n",
       "      <td>en</td>\n",
       "      <td>Aww</td>\n",
       "      <td>jbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272934</th>\n",
       "      <td>ISLAMIC TERRORIST IN THE HOUSE !</td>\n",
       "      <td>en</td>\n",
       "      <td>ISLAMIC TERRORIST IN THE HOUSE</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "645139                                               No!!!       en   \n",
       "2642386                          Very intelligent man !!!!       en   \n",
       "211820                                Lmao Uzair Piotr Bee       en   \n",
       "611778         https://www.youtube.com/watch?v=xI3sA004xlQ       en   \n",
       "331739                                       Ashley Larson       en   \n",
       "1829581  NO  MOTHER  FUCKER  NEVER DID FOIIOW  YOU  NOT...       en   \n",
       "1676961                                           Fuck no.       en   \n",
       "1312799                                              \"Bro\"       en   \n",
       "1528138                                               Aww.       en   \n",
       "1272934                   ISLAMIC TERRORIST IN THE HOUSE !       en   \n",
       "\n",
       "                                    comment_message_nopunc nopunc_language  \n",
       "645139                                                 No               es  \n",
       "2642386                              Very intelligent man               de  \n",
       "211820                                Lmao Uzair Piotr Bee              pl  \n",
       "611778                                       v xI3sA004xlQ              ru  \n",
       "331739                                       Ashley Larson              sv  \n",
       "1829581  NO MOTHER FUCKER NEVER DID FOIIOW YOU NOT NEVE...              ja  \n",
       "1676961                                           Fuck no               it  \n",
       "1312799                                               Bro               fr  \n",
       "1528138                                               Aww              jbo  \n",
       "1272934                    ISLAMIC TERRORIST IN THE HOUSE               ja  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of nopunc_lang_diff...\n",
    "# The specific results for those originally predicted English\n",
    "# punctuations removal ISN'T favorable\n",
    "comment.loc[nopunc_lang_diff & cmt_en, cmt_lang_cols_nopunc].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "streaming-enhancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_nopunc</th>\n",
       "      <th>nopunc_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2842033</th>\n",
       "      <td>R.I.P.</td>\n",
       "      <td>es</td>\n",
       "      <td>R I P</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548359</th>\n",
       "      <td>Amen.....</td>\n",
       "      <td>pt</td>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719750</th>\n",
       "      <td>;)</td>\n",
       "      <td>el</td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542721</th>\n",
       "      <td>Nit wit.</td>\n",
       "      <td>la</td>\n",
       "      <td>Nit wit</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660641</th>\n",
       "      <td>Amen!</td>\n",
       "      <td>eo</td>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379953</th>\n",
       "      <td>Fuckin Obamuslima!</td>\n",
       "      <td>pt</td>\n",
       "      <td>Fuckin Obamuslima</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410166</th>\n",
       "      <td>R.I.P.</td>\n",
       "      <td>es</td>\n",
       "      <td>R I P</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720964</th>\n",
       "      <td>Amen!!</td>\n",
       "      <td>sv</td>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126069</th>\n",
       "      <td>Happy Birthday Condoleezza!</td>\n",
       "      <td>it</td>\n",
       "      <td>Happy Birthday Condoleezza</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791100</th>\n",
       "      <td>Blah.</td>\n",
       "      <td>id</td>\n",
       "      <td>Blah</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418231</th>\n",
       "      <td>Kyra Freestone-Viljoen</td>\n",
       "      <td>nl</td>\n",
       "      <td>Kyra Freestone Viljoen</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256693</th>\n",
       "      <td>WOW!!!</td>\n",
       "      <td>ja</td>\n",
       "      <td>WOW</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430589</th>\n",
       "      <td>Amen!</td>\n",
       "      <td>eo</td>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416634</th>\n",
       "      <td>well.... bye</td>\n",
       "      <td>pt</td>\n",
       "      <td>well bye</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838702</th>\n",
       "      <td>YES AMEN!</td>\n",
       "      <td>it</td>\n",
       "      <td>YES AMEN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     comment_message language       comment_message_nopunc  \\\n",
       "2842033                       R.I.P.       es                       R I P    \n",
       "2548359                    Amen.....       pt                        Amen    \n",
       "1719750                           ;)       el                                \n",
       "2542721                     Nit wit.       la                     Nit wit    \n",
       "660641                         Amen!       eo                        Amen    \n",
       "1379953           Fuckin Obamuslima!       pt           Fuckin Obamuslima    \n",
       "410166                        R.I.P.       es                       R I P    \n",
       "720964                        Amen!!       sv                        Amen    \n",
       "1126069  Happy Birthday Condoleezza!       it  Happy Birthday Condoleezza    \n",
       "1791100                        Blah.       id                        Blah    \n",
       "2418231       Kyra Freestone-Viljoen       nl       Kyra Freestone Viljoen   \n",
       "2256693                       WOW!!!       ja                         WOW    \n",
       "430589                         Amen!       eo                        Amen    \n",
       "416634                  well.... bye       pt                     well bye   \n",
       "2838702                    YES AMEN!       it                    YES AMEN    \n",
       "\n",
       "        nopunc_language  \n",
       "2842033              en  \n",
       "2548359              en  \n",
       "1719750              en  \n",
       "2542721              en  \n",
       "660641               en  \n",
       "1379953              en  \n",
       "410166               en  \n",
       "720964               en  \n",
       "1126069              en  \n",
       "1791100              en  \n",
       "2418231              en  \n",
       "2256693              en  \n",
       "430589               en  \n",
       "416634               en  \n",
       "2838702              en  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of nopunc_lang_diff...\n",
    "# The specific results for those originally predicted non-English\n",
    "comment.loc[nopunc_lang_diff & cmt_nopunc_en, cmt_lang_cols_nopunc].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-closer",
   "metadata": {},
   "source": [
    "#### 4. Punctuation Removal only for Uppercase comments (failed attempt)\n",
    "* Doesn't seem favorable either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "heated-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cases of nopunc_lang_diff...\n",
    "# The \"Union\" case: Remove punctuations only for uppercase comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "greater-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.loc[comment['comment_message'].str.isupper() & nopunc_lang_diff, 'nopunc_language'].apply(lambda x: x == 'en').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "continuous-seating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_nopunc</th>\n",
       "      <th>nopunc_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1453961</th>\n",
       "      <td>TRUMP WON OHIO!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP WON OHIO</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344938</th>\n",
       "      <td>YES !</td>\n",
       "      <td>ta</td>\n",
       "      <td>YES</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752525</th>\n",
       "      <td>TRUE!</td>\n",
       "      <td>ar</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500940</th>\n",
       "      <td>GOOD COUPLE</td>\n",
       "      <td>en</td>\n",
       "      <td>GOOD COUPLE</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521793</th>\n",
       "      <td>JOIN THE N.R.A.</td>\n",
       "      <td>ru</td>\n",
       "      <td>JOIN THE N R A</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424381</th>\n",
       "      <td>#BIAS</td>\n",
       "      <td>eu</td>\n",
       "      <td>BIAS</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450994</th>\n",
       "      <td>TREASON !!!!!!!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>TREASON</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229053</th>\n",
       "      <td>THE POWERS OF CHRIST COMPELS YOU</td>\n",
       "      <td>en</td>\n",
       "      <td>THE POWERS OF CHRIST COMPELS YOU</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429128</th>\n",
       "      <td>YES!!!! KILL KILL KILL!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>YES KILL KILL KILL</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683288</th>\n",
       "      <td>BARACK OBAMA IS THE GREATEST THREAT AMERICA HA...</td>\n",
       "      <td>en</td>\n",
       "      <td>BARACK OBAMA IS THE GREATEST THREAT AMERICA HA...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400980</th>\n",
       "      <td>TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUM...</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUM...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771751</th>\n",
       "      <td>BLACK LIES MATTER!</td>\n",
       "      <td>en</td>\n",
       "      <td>BLACK LIES MATTER</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024556</th>\n",
       "      <td>PEICE OF CRAP IDIOT. WASN'T WORTH A DAM TEARS ...</td>\n",
       "      <td>en</td>\n",
       "      <td>PEICE OF CRAP IDIOT WASN T WORTH A DAM TEARS A...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168107</th>\n",
       "      <td>LIE SOME MORE BILL, YOU GOT EM WHERE YOU WANT EM!</td>\n",
       "      <td>en</td>\n",
       "      <td>LIE SOME MORE BILL YOU GOT EM WHERE YOU WANT EM</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805521</th>\n",
       "      <td>LEAVE!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>LEAVE</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288900</th>\n",
       "      <td>NO! NO! NO!</td>\n",
       "      <td>ja</td>\n",
       "      <td>NO NO NO</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168651</th>\n",
       "      <td>TRUMP!!</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579610</th>\n",
       "      <td>RECLAIM AMERICA!    TRUMP / CARSON!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>RECLAIM AMERICA TRUMP CARSON</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762124</th>\n",
       "      <td>BENGHAZI! MURDER!</td>\n",
       "      <td>en</td>\n",
       "      <td>BENGHAZI MURDER</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911017</th>\n",
       "      <td>BIG FUCKING BABIES.</td>\n",
       "      <td>en</td>\n",
       "      <td>BIG FUCKING BABIES</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429009</th>\n",
       "      <td>TRUMP!</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577690</th>\n",
       "      <td>TRUMP LIES!</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP LIES</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349191</th>\n",
       "      <td>GOOD BYE!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>GOOD BYE</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785368</th>\n",
       "      <td>NEWS FLASH!!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>NEWS FLASH</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229192</th>\n",
       "      <td>EPISODE? DRUNK? DRUGGED?</td>\n",
       "      <td>en</td>\n",
       "      <td>EPISODE DRUNK DRUGGED</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801622</th>\n",
       "      <td>QUE BUENA FOTO.</td>\n",
       "      <td>es</td>\n",
       "      <td>QUE BUENA FOTO</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524252</th>\n",
       "      <td>FAKE FAKE FAKE AND SICK</td>\n",
       "      <td>en</td>\n",
       "      <td>FAKE FAKE FAKE AND SICK</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933845</th>\n",
       "      <td>WTH???!!#</td>\n",
       "      <td>de</td>\n",
       "      <td>WTH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879477</th>\n",
       "      <td>#VOTEFORTRUMP, #WOMENFORTRUMP, #TRUMP2016, #TE...</td>\n",
       "      <td>en</td>\n",
       "      <td>VOTEFORTRUMP WOMENFORTRUMP TRUMP2016 TEAMTRUM...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975377</th>\n",
       "      <td>STOP ALREADY !!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>STOP ALREADY</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507812</th>\n",
       "      <td>TRUMP 2016</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP 2016</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839138</th>\n",
       "      <td>\"AMEN\"</td>\n",
       "      <td>pt</td>\n",
       "      <td>AMEN</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501464</th>\n",
       "      <td>YES !</td>\n",
       "      <td>ta</td>\n",
       "      <td>YES</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674925</th>\n",
       "      <td>WHATTTTTTT!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>WHATTTTTTT</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194633</th>\n",
       "      <td>\"LETS MAKE AMERICA GREAT AGAIN\"</td>\n",
       "      <td>en</td>\n",
       "      <td>LETS MAKE AMERICA GREAT AGAIN</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350746</th>\n",
       "      <td>TRUMP 2016!!!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP 2016</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118771</th>\n",
       "      <td>ANS HOW IS YOUR CAREER GOING ?????</td>\n",
       "      <td>en</td>\n",
       "      <td>ANS HOW IS YOUR CAREER GOING</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661858</th>\n",
       "      <td>NO !!!</td>\n",
       "      <td>de</td>\n",
       "      <td>NO</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927767</th>\n",
       "      <td>NO.   NN</td>\n",
       "      <td>en</td>\n",
       "      <td>NO NN</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681999</th>\n",
       "      <td>PUKE!!��</td>\n",
       "      <td>en</td>\n",
       "      <td>PUKE ��</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896773</th>\n",
       "      <td>OMG!</td>\n",
       "      <td>en</td>\n",
       "      <td>OMG</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175503</th>\n",
       "      <td>R.I.P.</td>\n",
       "      <td>es</td>\n",
       "      <td>R I P</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405301</th>\n",
       "      <td>O-,O</td>\n",
       "      <td>ar</td>\n",
       "      <td>O O</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589318</th>\n",
       "      <td>PAY THEM!!!!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>PAY THEM</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450991</th>\n",
       "      <td>TRUMP PENCE</td>\n",
       "      <td>en</td>\n",
       "      <td>TRUMP PENCE</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398169</th>\n",
       "      <td>HILLARY</td>\n",
       "      <td>en</td>\n",
       "      <td>HILLARY</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303711</th>\n",
       "      <td>LMAO</td>\n",
       "      <td>en</td>\n",
       "      <td>LMAO</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401926</th>\n",
       "      <td>\"TRADER\"</td>\n",
       "      <td>en</td>\n",
       "      <td>TRADER</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792210</th>\n",
       "      <td>HUM?</td>\n",
       "      <td>ko</td>\n",
       "      <td>HUM</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856218</th>\n",
       "      <td>HELL NO!!!! WE GIVE AWAY ENOUGH SHIT ALREADY T...</td>\n",
       "      <td>en</td>\n",
       "      <td>HELL NO WE GIVE AWAY ENOUGH SHIT ALREADY TO FO...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "1453961                                 TRUMP WON OHIO!!!!       en   \n",
       "344938                                               YES !       ta   \n",
       "2752525                                              TRUE!       ar   \n",
       "500940                                         GOOD COUPLE       en   \n",
       "521793                                     JOIN THE N.R.A.       ru   \n",
       "1424381                                              #BIAS       eu   \n",
       "450994                     TREASON !!!!!!!!!!!!!!!!!!!!!!!       en   \n",
       "2229053                   THE POWERS OF CHRIST COMPELS YOU       en   \n",
       "429128                          YES!!!! KILL KILL KILL!!!!       en   \n",
       "683288   BARACK OBAMA IS THE GREATEST THREAT AMERICA HA...       en   \n",
       "400980   TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUM...       en   \n",
       "771751                                  BLACK LIES MATTER!       en   \n",
       "1024556  PEICE OF CRAP IDIOT. WASN'T WORTH A DAM TEARS ...       en   \n",
       "1168107  LIE SOME MORE BILL, YOU GOT EM WHERE YOU WANT EM!       en   \n",
       "2805521                                         LEAVE!!!!!       en   \n",
       "288900                                         NO! NO! NO!       ja   \n",
       "168651                                             TRUMP!!       en   \n",
       "579610             RECLAIM AMERICA!    TRUMP / CARSON!!!!!       en   \n",
       "2762124                                  BENGHAZI! MURDER!       en   \n",
       "1911017                                BIG FUCKING BABIES.       en   \n",
       "1429009                                             TRUMP!       en   \n",
       "577690                                         TRUMP LIES!       en   \n",
       "2349191                                      GOOD BYE!!!!!       en   \n",
       "785368                                    NEWS FLASH!!!!!!       en   \n",
       "2229192                           EPISODE? DRUNK? DRUGGED?       en   \n",
       "801622                                     QUE BUENA FOTO.       es   \n",
       "2524252                            FAKE FAKE FAKE AND SICK       en   \n",
       "1933845                                          WTH???!!#       de   \n",
       "879477   #VOTEFORTRUMP, #WOMENFORTRUMP, #TRUMP2016, #TE...       en   \n",
       "1975377                                  STOP ALREADY !!!!       en   \n",
       "2507812                                         TRUMP 2016       en   \n",
       "839138                                              \"AMEN\"       pt   \n",
       "1501464                                              YES !       ta   \n",
       "2674925                                     WHATTTTTTT!!!!       en   \n",
       "194633                     \"LETS MAKE AMERICA GREAT AGAIN\"       en   \n",
       "2350746                                  TRUMP 2016!!!!!!!       en   \n",
       "118771                  ANS HOW IS YOUR CAREER GOING ?????       en   \n",
       "661858                                              NO !!!       de   \n",
       "1927767                                           NO.   NN       en   \n",
       "1681999                                           PUKE!!��       en   \n",
       "896773                                                OMG!       en   \n",
       "1175503                                             R.I.P.       es   \n",
       "1405301                                               O-,O       ar   \n",
       "1589318                                     PAY THEM!!!!!!       en   \n",
       "1450991                                        TRUMP PENCE       en   \n",
       "1398169                                            HILLARY       en   \n",
       "2303711                                               LMAO       en   \n",
       "401926                                            \"TRADER\"       en   \n",
       "792210                                                HUM?       ko   \n",
       "856218   HELL NO!!!! WE GIVE AWAY ENOUGH SHIT ALREADY T...       en   \n",
       "\n",
       "                                    comment_message_nopunc nopunc_language  \n",
       "1453961                                    TRUMP WON OHIO               ja  \n",
       "344938                                                YES               en  \n",
       "2752525                                              TRUE               de  \n",
       "500940                                         GOOD COUPLE              ja  \n",
       "521793                                     JOIN THE N R A               ja  \n",
       "1424381                                               BIAS              fr  \n",
       "450994                                            TREASON               ja  \n",
       "2229053                   THE POWERS OF CHRIST COMPELS YOU              ja  \n",
       "429128                                 YES KILL KILL KILL               zh  \n",
       "683288   BARACK OBAMA IS THE GREATEST THREAT AMERICA HA...              ja  \n",
       "400980   TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUMP TRUM...              de  \n",
       "771751                                  BLACK LIES MATTER               ja  \n",
       "1024556  PEICE OF CRAP IDIOT WASN T WORTH A DAM TEARS A...              ja  \n",
       "1168107   LIE SOME MORE BILL YOU GOT EM WHERE YOU WANT EM               ja  \n",
       "2805521                                             LEAVE               ja  \n",
       "288900                                           NO NO NO               eo  \n",
       "168651                                              TRUMP               de  \n",
       "579610                       RECLAIM AMERICA TRUMP CARSON               ja  \n",
       "2762124                                   BENGHAZI MURDER               ru  \n",
       "1911017                                BIG FUCKING BABIES               ja  \n",
       "1429009                                             TRUMP               de  \n",
       "577690                                         TRUMP LIES               fr  \n",
       "2349191                                          GOOD BYE               ja  \n",
       "785368                                         NEWS FLASH               ja  \n",
       "2229192                             EPISODE DRUNK DRUGGED               ja  \n",
       "801622                                     QUE BUENA FOTO               pt  \n",
       "2524252                            FAKE FAKE FAKE AND SICK              ja  \n",
       "1933845                                               WTH               en  \n",
       "879477    VOTEFORTRUMP WOMENFORTRUMP TRUMP2016 TEAMTRUM...              ja  \n",
       "1975377                                      STOP ALREADY               ja  \n",
       "2507812                                         TRUMP 2016              de  \n",
       "839138                                               AMEN               it  \n",
       "1501464                                               YES               en  \n",
       "2674925                                        WHATTTTTTT               eo  \n",
       "194633                      LETS MAKE AMERICA GREAT AGAIN               ja  \n",
       "2350746                                        TRUMP 2016               de  \n",
       "118771                       ANS HOW IS YOUR CAREER GOING               ja  \n",
       "661858                                                 NO               eo  \n",
       "1927767                                              NO NN              de  \n",
       "1681999                                            PUKE ��              lt  \n",
       "896773                                                OMG               vi  \n",
       "1175503                                             R I P               en  \n",
       "1405301                                                O O              pt  \n",
       "1589318                                          PAY THEM               ja  \n",
       "1450991                                        TRUMP PENCE              ja  \n",
       "1398169                                            HILLARY              pt  \n",
       "2303711                                               LMAO              ja  \n",
       "401926                                             TRADER               ja  \n",
       "792210                                                HUM               pl  \n",
       "856218   HELL NO WE GIVE AWAY ENOUGH SHIT ALREADY TO FO...              ja  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of nopunc_lang_diff...\n",
    "# The \"Union\" case: Remove punctuations only for Uppercase Comments\n",
    "# Punctuations are actually helpful in language prediction! esp. for uppercase comments!\n",
    "comment.loc[comment['comment_message'].str.isupper() & nopunc_lang_diff, cmt_lang_cols_nopunc].sample(50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "posted-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105568"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the cases of lang_diff...\n",
    "# Remove punctuations for non-uppercase comments and check out the results\n",
    "comment.loc[~comment['comment_message'].str.isupper() & lang_diff, 'language'].apply(lambda x: x == 'en').sum()\n",
    "comment.loc[~comment['comment_message'].str.isupper() & nopunc_lang_diff, 'nopunc_language'].apply(lambda x: x == 'en').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "collective-equation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>language</th>\n",
       "      <th>comment_message_lower</th>\n",
       "      <th>lower2language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146740</th>\n",
       "      <td>Hahahahaha!</td>\n",
       "      <td>it</td>\n",
       "      <td>hahahahaha!</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506981</th>\n",
       "      <td>Douglas Brito</td>\n",
       "      <td>en</td>\n",
       "      <td>douglas brito</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19606</th>\n",
       "      <td>Sad RIP</td>\n",
       "      <td>ja</td>\n",
       "      <td>sad rip</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366247</th>\n",
       "      <td>Omg</td>\n",
       "      <td>sv</td>\n",
       "      <td>omg</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520728</th>\n",
       "      <td>Dios!</td>\n",
       "      <td>la</td>\n",
       "      <td>dios!</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630217</th>\n",
       "      <td>No</td>\n",
       "      <td>es</td>\n",
       "      <td>no</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816613</th>\n",
       "      <td>Aww costarricense &lt;3</td>\n",
       "      <td>es</td>\n",
       "      <td>aww costarricense &lt;3</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895627</th>\n",
       "      <td>Nina Sobi</td>\n",
       "      <td>en</td>\n",
       "      <td>nina sobi</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562136</th>\n",
       "      <td>Race traitor white cunt.</td>\n",
       "      <td>la</td>\n",
       "      <td>race traitor white cunt.</td>\n",
       "      <td>si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090302</th>\n",
       "      <td>HAAaaa!!</td>\n",
       "      <td>nl</td>\n",
       "      <td>haaaaa!!</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042724</th>\n",
       "      <td>Alisa Brown</td>\n",
       "      <td>en</td>\n",
       "      <td>alisa brown</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754530</th>\n",
       "      <td>Camilla Byriel Mathilde Raaholt Christensen Ka...</td>\n",
       "      <td>en</td>\n",
       "      <td>camilla byriel mathilde raaholt christensen ka...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364841</th>\n",
       "      <td>Corinna Egbert Guilherme Fusquine ��</td>\n",
       "      <td>en</td>\n",
       "      <td>corinna egbert guilherme fusquine ��</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886586</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890479</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736854</th>\n",
       "      <td>Lasse Olsen ��</td>\n",
       "      <td>en</td>\n",
       "      <td>lasse olsen ��</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623844</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760925</th>\n",
       "      <td>Natalie Rudolph</td>\n",
       "      <td>en</td>\n",
       "      <td>natalie rudolph</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368432</th>\n",
       "      <td>Fuck no</td>\n",
       "      <td>it</td>\n",
       "      <td>fuck no</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298740</th>\n",
       "      <td>Hell yes</td>\n",
       "      <td>ca</td>\n",
       "      <td>hell yes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461984</th>\n",
       "      <td>Derr!!</td>\n",
       "      <td>pt</td>\n",
       "      <td>derr!!</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832017</th>\n",
       "      <td>So sad</td>\n",
       "      <td>en</td>\n",
       "      <td>so sad</td>\n",
       "      <td>bs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277894</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909628</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818978</th>\n",
       "      <td>Linda Schneider</td>\n",
       "      <td>en</td>\n",
       "      <td>linda schneider</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801844</th>\n",
       "      <td>Dymek Oka</td>\n",
       "      <td>en</td>\n",
       "      <td>dymek oka</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804124</th>\n",
       "      <td>Evil</td>\n",
       "      <td>pl</td>\n",
       "      <td>evil</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192795</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452434</th>\n",
       "      <td>http://bit.ly/1pjH6Fu</td>\n",
       "      <td>ru</td>\n",
       "      <td>http://bit.ly/1pjh6fu</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817611</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041682</th>\n",
       "      <td>Kelly Lu</td>\n",
       "      <td>en</td>\n",
       "      <td>kelly lu</td>\n",
       "      <td>jbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445990</th>\n",
       "      <td>Lets Do This!! :)</td>\n",
       "      <td>en</td>\n",
       "      <td>lets do this!! :)</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237209</th>\n",
       "      <td>Ciara Smith Katie Killen phahaha</td>\n",
       "      <td>en</td>\n",
       "      <td>ciara smith katie killen phahaha</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108217</th>\n",
       "      <td>William Loloa</td>\n",
       "      <td>en</td>\n",
       "      <td>william loloa</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312892</th>\n",
       "      <td>Elisa Kattan</td>\n",
       "      <td>en</td>\n",
       "      <td>elisa kattan</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050328</th>\n",
       "      <td>Maria Bautista</td>\n",
       "      <td>es</td>\n",
       "      <td>maria bautista</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732792</th>\n",
       "      <td>Come to texas.</td>\n",
       "      <td>en</td>\n",
       "      <td>come to texas.</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435019</th>\n",
       "      <td>Amy McCusker</td>\n",
       "      <td>en</td>\n",
       "      <td>amy mccusker</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69756</th>\n",
       "      <td>Dan Freeman Tony Lynas</td>\n",
       "      <td>en</td>\n",
       "      <td>dan freeman tony lynas</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138907</th>\n",
       "      <td>Deā Navtej Singh</td>\n",
       "      <td>en</td>\n",
       "      <td>deā navtej singh</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329872</th>\n",
       "      <td>Vivian</td>\n",
       "      <td>en</td>\n",
       "      <td>vivian</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205432</th>\n",
       "      <td>Amen</td>\n",
       "      <td>en</td>\n",
       "      <td>amen</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744789</th>\n",
       "      <td>Jofer Valencia</td>\n",
       "      <td>en</td>\n",
       "      <td>jofer valencia</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400782</th>\n",
       "      <td>Idiot</td>\n",
       "      <td>ru</td>\n",
       "      <td>idiot</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614793</th>\n",
       "      <td>Kelly Batey</td>\n",
       "      <td>en</td>\n",
       "      <td>kelly batey</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626515</th>\n",
       "      <td>Ughhhhhh!</td>\n",
       "      <td>en</td>\n",
       "      <td>ughhhhhh!</td>\n",
       "      <td>hu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183043</th>\n",
       "      <td>Terri Mccormick Ellis. Ohhh myyy.</td>\n",
       "      <td>fi</td>\n",
       "      <td>terri mccormick ellis. ohhh myyy.</td>\n",
       "      <td>hu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225705</th>\n",
       "      <td>Natalie Mikaelian Grip</td>\n",
       "      <td>en</td>\n",
       "      <td>natalie mikaelian grip</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95075</th>\n",
       "      <td>Yehia Abudiba</td>\n",
       "      <td>en</td>\n",
       "      <td>yehia abudiba</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847261</th>\n",
       "      <td>Dan Reason</td>\n",
       "      <td>en</td>\n",
       "      <td>dan reason</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_message language  \\\n",
       "146740                                         Hahahahaha!       it   \n",
       "2506981                                      Douglas Brito       en   \n",
       "19606                                              Sad RIP       ja   \n",
       "366247                                                 Omg       sv   \n",
       "1520728                                              Dios!       la   \n",
       "630217                                                  No       es   \n",
       "1816613                               Aww costarricense <3       es   \n",
       "895627                                           Nina Sobi       en   \n",
       "1562136                           Race traitor white cunt.       la   \n",
       "1090302                                           HAAaaa!!       nl   \n",
       "1042724                                        Alisa Brown       en   \n",
       "1754530  Camilla Byriel Mathilde Raaholt Christensen Ka...       en   \n",
       "1364841               Corinna Egbert Guilherme Fusquine ��       en   \n",
       "886586                                                Amen       en   \n",
       "1890479                                               Amen       en   \n",
       "1736854                                     Lasse Olsen ��       en   \n",
       "2623844                                               Amen       en   \n",
       "2760925                                    Natalie Rudolph       en   \n",
       "368432                                             Fuck no       it   \n",
       "298740                                            Hell yes       ca   \n",
       "1461984                                             Derr!!       pt   \n",
       "2832017                                             So sad       en   \n",
       "1277894                                               Amen       en   \n",
       "1909628                                               Amen       en   \n",
       "2818978                                    Linda Schneider       en   \n",
       "801844                                           Dymek Oka       en   \n",
       "2804124                                               Evil       pl   \n",
       "2192795                                               Amen       en   \n",
       "1452434                              http://bit.ly/1pjH6Fu       ru   \n",
       "1817611                                               Amen       en   \n",
       "2041682                                           Kelly Lu       en   \n",
       "2445990                                  Lets Do This!! :)       en   \n",
       "1237209                   Ciara Smith Katie Killen phahaha       en   \n",
       "1108217                                      William Loloa       en   \n",
       "1312892                                       Elisa Kattan       en   \n",
       "1050328                                     Maria Bautista       es   \n",
       "2732792                                     Come to texas.       en   \n",
       "1435019                                       Amy McCusker       en   \n",
       "69756                               Dan Freeman Tony Lynas       en   \n",
       "1138907                                   Deā Navtej Singh       en   \n",
       "2329872                                             Vivian       en   \n",
       "1205432                                               Amen       en   \n",
       "2744789                                     Jofer Valencia       en   \n",
       "400782                                               Idiot       ru   \n",
       "1614793                                        Kelly Batey       en   \n",
       "2626515                                          Ughhhhhh!       en   \n",
       "1183043                  Terri Mccormick Ellis. Ohhh myyy.       fi   \n",
       "2225705                             Natalie Mikaelian Grip       en   \n",
       "95075                                        Yehia Abudiba       en   \n",
       "847261                                          Dan Reason       en   \n",
       "\n",
       "                                     comment_message_lower lower2language  \n",
       "146740                                         hahahahaha!             de  \n",
       "2506981                                      douglas brito             de  \n",
       "19606                                              sad rip             it  \n",
       "366247                                                 omg             nl  \n",
       "1520728                                              dios!             el  \n",
       "630217                                                  no             pt  \n",
       "1816613                               aww costarricense <3             pt  \n",
       "895627                                           nina sobi             es  \n",
       "1562136                           race traitor white cunt.             si  \n",
       "1090302                                           haaaaa!!             ru  \n",
       "1042724                                        alisa brown             es  \n",
       "1754530  camilla byriel mathilde raaholt christensen ka...             no  \n",
       "1364841               corinna egbert guilherme fusquine ��             pt  \n",
       "886586                                                amen             ca  \n",
       "1890479                                               amen             ca  \n",
       "1736854                                     lasse olsen ��             it  \n",
       "2623844                                               amen             ca  \n",
       "2760925                                    natalie rudolph             de  \n",
       "368432                                             fuck no             pt  \n",
       "298740                                            hell yes             de  \n",
       "1461984                                             derr!!             ja  \n",
       "2832017                                             so sad             bs  \n",
       "1277894                                               amen             ca  \n",
       "1909628                                               amen             ca  \n",
       "2818978                                    linda schneider             de  \n",
       "801844                                           dymek oka             pl  \n",
       "2804124                                               evil             fr  \n",
       "2192795                                               amen             ca  \n",
       "1452434                              http://bit.ly/1pjh6fu             de  \n",
       "1817611                                               amen             ca  \n",
       "2041682                                           kelly lu            jbo  \n",
       "2445990                                  lets do this!! :)             pt  \n",
       "1237209                   ciara smith katie killen phahaha             nl  \n",
       "1108217                                      william loloa             de  \n",
       "1312892                                       elisa kattan             sv  \n",
       "1050328                                     maria bautista             de  \n",
       "2732792                                     come to texas.             it  \n",
       "1435019                                       amy mccusker             sh  \n",
       "69756                               dan freeman tony lynas             id  \n",
       "1138907                                   deā navtej singh             nl  \n",
       "2329872                                             vivian             es  \n",
       "1205432                                               amen             ca  \n",
       "2744789                                     jofer valencia             es  \n",
       "400782                                               idiot             es  \n",
       "1614793                                        kelly batey             pt  \n",
       "2626515                                          ughhhhhh!             hu  \n",
       "1183043                  terri mccormick ellis. ohhh myyy.             hu  \n",
       "2225705                             natalie mikaelian grip             fr  \n",
       "95075                                        yehia abudiba             ms  \n",
       "847261                                          dan reason             id  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.loc[~comment['comment_message'].str.isupper() & lang_diff, cmt_lang_cols].sample(50, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-platinum",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing-comment-data\"></a>\n",
    "## Preprocessing Comment Data\n",
    "\n",
    "[[Back to Navigation]](#navigation)\n",
    "\n",
    "#### Minimum Preprocessing\n",
    "1. Drop completely duplicate rows \n",
    "2. Add language labels (using Language Detection Model from fasttext)\n",
    " * the reason I use fasttext: [Benchmarking Language Detection for NLP](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c)\n",
    " * [PyPI for fasttext](https://pypi.org/project/fasttext/)\n",
    " * [medium on fasttext](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)\n",
    " * Meanwhile, Summarize the number of English, non-English, and empty comments, and store the summary in a global counter, `en_counter_comment`\n",
    "3. Filter out non-English or empty comments; then, drop the language column\n",
    "\n",
    "#### Further Preprocessing\n",
    "4. Convert all words to lowercase\n",
    "5. Expand contractions and slangs (eg. yall're cool -> you all are cool): [GitHub for contractions](https://github.com/kootenpv/contractions)\n",
    "6. Remove html `<br>` tags, punctuations, links, newlines, tabs, and shrink consecutive spaces to one\n",
    "7. Remove emojis\n",
    "8. Remove stopwords (using nltk)\n",
    "9. Lemmatize texts (using spacy)\n",
    "10. Pickle (Serialize) the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "behavioral-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decorator to decorate text-transformation functions\n",
    "def func_to_string(func):\n",
    "    \"\"\"\n",
    "    A decorator that allows a function to bypass nonstring arguments. That is, func applies only to string columns.\n",
    "    \"\"\"\n",
    "    def wrapper_func(x):\n",
    "        try:\n",
    "            return func(x)\n",
    "        except:\n",
    "            return x\n",
    "    return wrapper_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virgin-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# Functions to apply to textual column\n",
    "@func_to_string\n",
    "def contract_message(x):\n",
    "    return contractions.fix(x)\n",
    "\n",
    "pretrained_model = \"/home3/r09725056/.conda/envs/usfb/lib/python3.7/site-packages/fasttext/lid.176.bin\"\n",
    "model = fasttext.load_model(pretrained_model)    \n",
    "@func_to_string\n",
    "def predict_language(sent):\n",
    "    sent = sent.replace('\\n', ' ')\n",
    "    pred = model.predict(sent) # model.predict() returns a tuple like this: (('__label__en',), array([0.95346403]))\n",
    "    return pred[0][0].split('_')[-1]\n",
    "\n",
    "@func_to_string\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "print(nlp.pipe_names)\n",
    "@func_to_string\n",
    "def lemmatize(text):\n",
    "    return \" \".join(token.lemma_ for token in nlp(text))\n",
    "\n",
    "@func_to_string\n",
    "def remove_punc_symbol(text):\n",
    "    \"Remove html <br> tags, links, newlines, tabs, punctuations, and shrink spaces\"\n",
    "    # Remove html <br> tags, links, newlines, tabs\n",
    "    pattern = re.compile(r'(<br>|https?://[a-zA-Z0-9-./|\\s]+)', re.IGNORECASE)\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    # Remove puntuations\n",
    "    mapping_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(mapping_table)\n",
    "    # Shrink spaces\n",
    "    text = re.sub(r'[ ]+', ' ', text) # replace multiple spaces with one space\n",
    "    return text\n",
    "\n",
    "stop = set(stopwords.words('english')) # Get stopwords from nltk\n",
    "@func_to_string\n",
    "def remove_stopwords(text):\n",
    "    x = text\n",
    "    return \" \".join(x for x in x.split() if x not in stop) # remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removed-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-defined functions\n",
    "def check_lang_code(code):\n",
    "    with open('temp/temp_variables/lang_code.pkl', 'rb') as f:\n",
    "        lang_code = pickle.load(f)\n",
    "    return lang_code.loc[lang_code['639-1'] == code, 'ISO language name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A global counter for language\n",
    "en_counter_comment = {'en': 0, 'non_en': 0, 'null': 0}\n",
    "\n",
    "def preprocess_pickle_comments(remove_stop=True, lemmatize=True) -> None:\n",
    "    \"Minimum & Further Preprocessing\"\n",
    "    # Set source and destination path\n",
    "    src_path = r'input/comment/2015-01-01-to-2016-11-30'\n",
    "    dest_path = r'temp/comment/preprocessed'\n",
    "    # Loop for each file in src_path\n",
    "    for i, f_name in enumerate(sorted(os.listdir(src_path))):\n",
    "        # Set source and destination files\n",
    "        src_file = os.path.join(src_path, f_name)\n",
    "        dest_file = dest_file = os.path.join(dest_path, f'comment_{i}.pkl')\n",
    "        # Read in Data\n",
    "        usecols = ['comment_message', 'post_id', 'comment_created_time']\n",
    "        temp = pd.read_csv(src_file, parse_dates=['comment_created_time'], usecols=usecols)\n",
    "        # Preprocessing\n",
    "        temp.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        temp['language'] = temp['comment_message'].apply(predict_language) # Add language labels\n",
    "        # Update language counts\n",
    "        total = len(temp)\n",
    "        n_null = temp['language'].value_counts()[1:].sum()\n",
    "        n_en = temp['language'].value_counts()['en']\n",
    "        n_non_en = total - n_null - n_en\n",
    "        en_counter_comment['en'] += n_en # update english\n",
    "        en_counter_comment['non_en'] += n_non_en # update non-english\n",
    "        en_counter_comment['null'] += n_null # update null values\n",
    "        # Preprocessing (part 2)\n",
    "        temp = temp[temp['language'] == 'en'] # filter out non-English and NaN\n",
    "        temp.drop(columns='language', inplace=True) # drop the language column\n",
    "        temp['comment_message'] = temp['comment_message'].str.lower() # set all words to lowercase\n",
    "        temp['comment_message'] = temp['comment_message'].apply(contract_message) # expand contractions\n",
    "        temp['comment_message'] = temp['comment_message'].apply(remove_punc_symbol) # remove punctuations and special characters\n",
    "        temp['comment_message'] = temp['comment_message'].apply(lambda x: remove_emoji(x)) # remove emojis\n",
    "        if remove_stop:\n",
    "            stop = set(stopwords.words('english')) # Get stopwords from nltk\n",
    "            temp['comment_message'] = temp['comment_message'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) # remove stopwords\n",
    "        if lemmatize:\n",
    "            try: # lemmatize words (in a faster way)\n",
    "                temp['comment_message'] = [' '.join(token.lemma_ for token in doc) for doc in nlp.pipe(temp['comment_message'])]\n",
    "            except TypeError: # lemmatize in a slower way (which accepts null values)\n",
    "                temp['comment_message'] = temp['comment_message'].apply(lemmatize)\n",
    "        # Pickling\n",
    "        temp.to_pickle(dest_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fatty-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    preprocess_pickle_comments() # DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result of the global counter\n",
    "with open('temp/temp_variables/en_counter_comment.pkl') as f:\n",
    "    pickle.dump(en_counter_comment, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "qualified-compromise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 418313529, 'non_en': 9717226, 'null': 50211963}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_counter_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-offset",
   "metadata": {},
   "source": [
    "<a id=\"dealing-with-contractions\"></a>\n",
    "\n",
    "## Dealing with Contractions...\n",
    "[[Back to Navigation]](#navigation)\n",
    "\n",
    "Some related sources about contractions:\n",
    "* [Common Informal Contractions in English You Need to Know](https://www.fluentland.com/common-informal-contractions/)<br>\n",
    "* [50個超常用的簡訊縮寫](https://tw.blog.voicetube.com/archives/41825/9-posctn-ttyl-k-%E5%88%B0%E5%BA%95%E6%98%AF%E5%95%A5%E6%84%8F%E6%80%9D%EF%BC%9F50%E5%80%8B%E8%B6%85%E5%B8%B8%E7%94%A8%E7%9A%84%E7%B0%A1%E8%A8%8A%E7%B8%AE%E5%AF%AB/)\n",
    "* [internetslang.com](https://www.internetslang.com/list.asp?i=all) ([scraped - my code](#scrape))\n",
    "\n",
    "I have made full use of the above resources to expand my contraction dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_my_contractions(mapping_dict):\n",
    "    with open('temp/temp_variables/my_contractions.pkl', 'rb') as read_f:\n",
    "        my_contractions = pickle.load(read_f)\n",
    "    for slang, meaning in mapping_dict.items():\n",
    "        if slang not in my_contractions:\n",
    "            slang = slang.lower()\n",
    "            meaning = meaning.lower()\n",
    "            my_contractions[slang] = meaning\n",
    "            print(f\"({slang}: {meaning}) is successfully added.\")\n",
    "        else:\n",
    "            print(f\"{slang.lower()} is included already...\")\n",
    "    with open('temp/temp_variables/my_contractions.pkl', 'wb') as write_f:\n",
    "        pickle.dump(my_contractions, write_f)\n",
    "# Set (slang: meaning) dictionary\n",
    "mapping_dict = {\n",
    "    \"whatcha\": \"what have you\" # I have manually updated contractions whenever I see new ones...\n",
    "}\n",
    "# Update!!!\n",
    "update_my_contractions(mapping_dict) # Save result on disk\n",
    "with open('temp/temp_variables/my_contractions.pkl', 'rb') as read_f:\n",
    "    my_contractions = pickle.load(read_f)\n",
    "for slang, meaning in my_contractions.items():\n",
    "    contractions.add(slang, meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-finding",
   "metadata": {},
   "source": [
    "<a id=\"scrape\"></a>\n",
    "#### This is the piece of code with which I wrote to scrape slangs and their meanings from internetslang.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_page(path, verbose=False):\n",
    "    # Get the webpage\n",
    "    try:\n",
    "        html = requests.get(path)\n",
    "    except HTTPError as e:\n",
    "        # couldn't find the webpage on the server\n",
    "        print(e)\n",
    "    except URLError as e:\n",
    "        # More severe problem: cannot find the server\n",
    "        print('The server could not be found!')\n",
    "    except Exception as e:\n",
    "        # Other unknown problems\n",
    "        print('Sorry, something went wrong...')\n",
    "        print(e)\n",
    "    else:\n",
    "        # Test if no exception are thrown\n",
    "        if verbose:\n",
    "            print('page successfully retrieved!')\n",
    "        \n",
    "    return html\n",
    "\n",
    "def strip_replace(meaning_list):\n",
    "    return [m.strip().replace('&amp;', '&') for m in meaning_list]\n",
    "\n",
    "def create_slang_df(full_slang_list_untackled):\n",
    "    df = pd.DataFrame(full_slang_list_untackled, columns=['slang', 'meaning', 'untackled'])\n",
    "    df.drop_duplicates(subset='slang', inplace=True, ignore_index=False)\n",
    "    # Add important slangs that're not scraped\n",
    "    my_slang = [('WTF', ['What the Fuck?'], False), \\\n",
    "                (\"YW\", [\"You're Welcome\"], False), \\\n",
    "                ('SMH', ['Shaking my head. Used to express shock or disappointment.'], False)\n",
    "               ]\n",
    "    my_slang = pd.DataFrame(my_slang, columns=df.columns)\n",
    "    df = pd.concat([df, my_slang], ignore_index=True)\n",
    "    df.loc[df.untackled, 'new_meaning'] = df.loc[df.untackled, 'meaning'].apply(lambda x: get_meaning(x[0]))\n",
    "    df.loc[~df.untackled, 'new_meaning'] = df.loc[~df.untackled, 'meaning']\n",
    "    return df\n",
    "\n",
    "def scrape_slangs(path, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples in the form of (slang, meaning, untackled)\n",
    "    \n",
    "    Return:\n",
    "    ----------------------------------------------\n",
    "    slang: str\n",
    "        The word to look up\n",
    "    meaning: list\n",
    "        a list of possible meanings (str)\n",
    "    untackled: bool\n",
    "        equals True if `meaning` still contains slangs/contractions. \n",
    "        This is caused by the website adding link(s) in the meaning section instead of its meaning(s) directly.\n",
    "    \"\"\"\n",
    "    # Get the webpage\n",
    "    html = get_page(path)\n",
    "    # Create a BeautifulSoup Object from the webpage\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    # Get all slangs & meanings, and store them in rows\n",
    "    pattern = re.compile(r'nodepath:/html/body/table/tr\\[1\\]/td/table\\[3\\]/tr\\[\\d+\\];display:table-row;')\n",
    "    rows = soup.find_all('tr', style=pattern)\n",
    "    \n",
    "    if path.endswith('=1'): # The first page\n",
    "        rows = rows[4:] # Get rid of the first four rows\n",
    "\n",
    "    # Store all slangs and their meanings in slang_list\n",
    "    slang_list = []\n",
    "    for row in rows:\n",
    "        # Get the slang\n",
    "        slang = row.find('a').text\n",
    "        if slang == 'All': # Not slang (but page link instead)\n",
    "            break\n",
    "        # Parse out the meaning & Add a label (UNTACKLED)\n",
    "        meaning_section = row.find_all('td')[-1]\n",
    "        untackled = False\n",
    "        tags_contained = meaning_section.find_all(True)\n",
    "        if tags_contained: # meaning_section contains at least a tag\n",
    "            tag_names = [tag.name for tag in tags_contained]\n",
    "            if 'a' in tag_names: # contains <a> tag\n",
    "                meaning_list = [meaning_section.find('a').decode_contents()]\n",
    "                # Mark the meaning_list as \"UNTACKLED\"; will tackle these later by look them up\n",
    "                untackled = True\n",
    "            elif 'br' in tag_names: # contains <br> tag\n",
    "                meaning_list = meaning_section.decode_contents().split('<br/>')\n",
    "                meaning_list = [m for m in meaning_list]\n",
    "        else: # meaning_section doesn't contain any tag\n",
    "            meaning_list = [meaning_section.decode_contents()] # with length of one\n",
    "        # Preprocess each meaning in the meaning_list\n",
    "        meaning_list = strip_replace(meaning_list)\n",
    "        # Store the results\n",
    "        vocab = (slang, meaning_list, untackled)\n",
    "        slang_list.append(vocab)\n",
    "        # Print for testing\n",
    "        if verbose:\n",
    "            print(\"{}: {}, UNTACKLED={}\".format(*vocab))\n",
    "    # End of For Loop\n",
    "    return slang_list\n",
    "\n",
    "def first_scrape():\n",
    "    \"Scrape all slangs for the first time\"\n",
    "    full_slang_list = []\n",
    "    for i in range(1, 26):\n",
    "        path = f'https://www.internetslang.com/list.asp?i=all&ezpage={i}'\n",
    "        slang_list = scrape_slangs(path, verbose=False) # Change to True to see the results in real time\n",
    "        full_slang_list.extend(slang_list)\n",
    "    return full_slang_list\n",
    "\n",
    "def main() -> None:\n",
    "    os.chdir('/home3/r09725056/Desktop/analysis-ChingYaoL')\n",
    "    full_slang_list_untackled = first_scrape()\n",
    "    with open(r'temp/temp_variables/full_slang_list_untackled.pkl', 'wb') as f:\n",
    "        pickle.dump(full_slang_list_untackled, f)\n",
    "        \n",
    "    slang_df = create_slang_df(full_slang_list_untackled)\n",
    "    with open(r'temp/temp_variables/slang_df.pkl', 'wb') as f:\n",
    "        pickle.dump(slang_df, f)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-pacific",
   "metadata": {},
   "source": [
    "<a id=\"add-spacytextblob\"></a>\n",
    "\n",
    "## Add spacyTextblob Sentiment Polarity and Subjectivity\n",
    "[[Back to Navigation]](#navigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fifty-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fifth-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7f3a481f94d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'spacytextblob']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-outdoors",
   "metadata": {},
   "source": [
    "### Quick test\n",
    "I discovered that `nlp.pipe` runs faster on pickled data than on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "after-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cmt = pd.read_pickle(f'temp/comment/preprocessed/comment_{i}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "extra-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5 s ± 123 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [(doc._.polarity, doc._.subjectivity, doc._.assessments) for doc in nlp.pipe(cmt['comment_message'].iloc[:10000])]\n",
    "# 15.5 s ± 123 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "satellite-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(f'temp/comment/original/comment_{i}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "broadband-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2 s ± 147 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [(doc._.polarity, doc._.subjectivity, doc._.assessments) for doc in nlp.pipe(test['comment_message'].iloc[:10000])]\n",
    "# 28.2 s ± 147 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-penny",
   "metadata": {},
   "source": [
    "#### Thus, in the function below, I would use `nlp.pipe` in combination with pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_to_preprocessed_comment() -> None:\n",
    "    \"Calculate sentiment for each comment (preprocessed), and dump to new directory\"\n",
    "    for i in range(500):\n",
    "        cmt = pd.read_pickle(f'temp/comment/preprocessed/comment_{i}.pkl') # Use pickled data\n",
    "        # Create new columns\n",
    "        cmt[['polarity', 'subjectivity', 'assessments']] = [(doc._.polarity, doc._.subjectivity, doc._.assessments) for doc in nlp.pipe(cmt['comment_message'])]\n",
    "        cmt.to_pickle(f'temp/comment/preprocessed_with_sentiment/comment_{i}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-greek",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    add_sentiment_to_preprocessed_comment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-massachusetts",
   "metadata": {},
   "source": [
    "<a id=\"post-data\"></a>\n",
    "\n",
    "# Post\n",
    "Data stored in the variable: top_1000_pages\n",
    "\n",
    "[[Back to Navigation]](#navigation)\n",
    "\n",
    "### Techniques Adopted for Cleaning Post Texts\n",
    "\n",
    "##### Basic Preprocessing\n",
    "0. Drop unwanted columns ('post_created_date_CT', 'post_updated_date_CT', 'post_created_time', 'post_updated_time')\n",
    "1. Drop completely duplicate rows\n",
    "2. Expand contractions and slangs (eg. yall're cool -> you all are cool): [GitHub for contractions](https://github.com/kootenpv/contractions)\n",
    "3. Add language labels (using Language Detection Model from fasttext)\n",
    " * the reason I use fasttext: [Benchmarking Language Detection for NLP](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c)\n",
    " * [PyPI for fasttext](https://pypi.org/project/fasttext/)\n",
    " * [medium on fasttext](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)\n",
    "4. Summarize the number of English, non-English, and empty comments, and store the summary in a global counter, `en_counter_post`\n",
    "5. Filter out non-English or empty comments; then, drop the language column\n",
    "6. Convert all words to lowercase\n",
    "7. Remove html `<br>` tags, punctuations, links, newlines, tabs, and shrink consecutive spaces to one\n",
    "8. Remove emojis\n",
    "9. Remove stopwords (using nltk)\n",
    "10. Lemmatize texts (using spacy)\n",
    "11. Pickle (Serialize) the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the original top_1000_pages data\n",
    "top_1000_pages = pd.read_csv(r'input/post/1000-page/2015-01-01-to-2017-04-08.csv', parse_dates=['post_created_time_CT', 'post_updated_time_CT', 'post_created_date_CT', 'post_updated_date_CT', 'post_created_time', 'post_updated_time'])\n",
    "top_1000_pages.to_pickle(r'temp/post/original/top_1000_pages.pkl') # DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A global counter for language\n",
    "en_counter_post = {'en': 0, 'non_en': 0, 'null': 0}\n",
    "\n",
    "def preprocess_pickle_post(remove_stop=True, lemmatize=True) -> None:\n",
    "    # Set source and destination path\n",
    "    src_path = r'temp/post/original'\n",
    "    dest_path = r'temp/post/preprocessed'\n",
    "    # Set source and destination files\n",
    "    src_file = os.path.join(src_path, 'top_1000_pages.pkl')\n",
    "    dest_file = dest_file = os.path.join(dest_path, 'top_1000_page_preprocessed.pkl')\n",
    "    # Read in Data\n",
    "    temp = pd.read_pickle(src_file)\n",
    "    # Preprocessing\n",
    "    temp.drop(columns=['post_created_date_CT', 'post_updated_date_CT', 'post_created_time', 'post_updated_time'], inplace=True)\n",
    "    temp.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    temp['post_message_converted'] = temp['post_message'].apply(contract_message) # expand contractions\n",
    "    temp['language'] = temp['post_message'].apply(predict_language) # Add language labels\n",
    "    # Update language counts\n",
    "    total = len(temp)\n",
    "    n_null = temp['language'].value_counts()[1:].sum()\n",
    "    n_en = temp['language'].value_counts()['en']\n",
    "    n_non_en = total - n_null - n_en\n",
    "    en_counter_post['en'] += n_en # update english\n",
    "    en_counter_post['non_en'] += n_non_en # update non-english\n",
    "    en_counter_post['null'] += n_null # update null values\n",
    "    # Preprocessing (part 2)\n",
    "    temp = temp[temp['language'] == 'en'] # filter out non-English and NaN\n",
    "    temp.drop(columns='language', inplace=True) # drop the language column\n",
    "    temp['post_message_converted'] = temp['post_message_converted'].str.lower() # set all words to lowercase\n",
    "    temp['post_message_converted'] = temp['post_message_converted'].apply(remove_punc_symbol) # remove punctuations and special characters\n",
    "    temp['post_message_converted'] = temp['post_message_converted'].apply(lambda x: remove_emoji(x)) # remove emojis\n",
    "    if remove_stop:\n",
    "        stop = set(stopwords.words('english')) # Get stopwords from nltk\n",
    "        temp['post_message_converted'] = temp['post_message_converted'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) # remove stopwords\n",
    "    if lemmatize:\n",
    "        try: # lemmatize words (in a faster way)\n",
    "            temp['post_message_converted'] = [' '.join(token.lemma_ for token in doc) for doc in nlp.pipe(temp['post_message_converted'])]\n",
    "        except TypeError: # lemmatize in a slower way (which accepts null values)\n",
    "            temp['post_message_converted'] = temp['post_message_converted'].apply(lemmatize)\n",
    "    # Pickling\n",
    "    temp.to_pickle(dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    preprocess_pickle_post() # DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp/temp_variables/en_counter_post.pkl', 'wb') as f:\n",
    "    pickle.dump(en_counter_post, f) # DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-netscape",
   "metadata": {},
   "source": [
    "### Graph: Number of Samples for Top Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of unwanted columns\n",
    "cols_to_drop = ['post_picture', 'post_link', 'post_created_time', 'post_updated_time']\n",
    "top_1000_pages.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['post_message'] = top_1000_pages['post_message'].apply(contract_message)\n",
    "top_1000_pages['language'] = top_1000_pages['post_message'].apply(predict_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-radiation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the results\n",
    "# print(f\"{top_1000_pages.post_name.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_message.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_caption.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_description.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_reactions.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_likes.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_comments.isna().mean():.6f}\")\n",
    "# print(f\"{top_1000_pages.post_shares.isna().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_unique_elements(ser, thres=0.001, ndecimal=6, show_null_rate=True):\n",
    "    \"\"\"\n",
    "    Return top unique elements which take up no less than 0.001 (or thres) in the entire pd.Series\n",
    "    This is done through ser.value_counts(normalize=True, dropna=False)\n",
    "    \"\"\"\n",
    "    filt = (ser.value_counts(dropna=False, normalize=True) >= thres)\n",
    "    if show_null_rate:\n",
    "        print(f\"null rate = {ser.isnull().mean() * 100:.6f} %\")\n",
    "    return ser.value_counts(dropna=False).loc[filt].round(ndecimal)\n",
    "\n",
    "# filter for languages that's at least 0.1%, then assign to top_languages\n",
    "lang_thres = 0.001\n",
    "top_languages = filter_top_unique_elements(top_1000_pages['language'], lang_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler way to format yticks\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# f = mticker.ScalarFormatter(useOffset=False, useMathText=True)\n",
    "# g = lambda x, pos : \"${}$\".format(f._formatSciNotation('%1.10e' % x))\n",
    "# plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(g))\n",
    "# top_languages.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Customize Yticks\n",
    "class MathTextSciFormatter(mticker.Formatter):\n",
    "    def __init__(self, fmt=\"%1.2e\"):\n",
    "        self.fmt = fmt\n",
    "    def __call__(self, x, pos=None):\n",
    "        s = self.fmt % x\n",
    "        decimal_point = '.'\n",
    "        positive_sign = '+'\n",
    "        tup = s.split('e')\n",
    "        significand = tup[0].rstrip(decimal_point)\n",
    "        sign = tup[1][0].replace(positive_sign, '')\n",
    "        exponent = tup[1][1:].lstrip('0')\n",
    "        if exponent:\n",
    "            exponent = '10^{%s%s}' % (sign, exponent)\n",
    "        if significand and exponent:\n",
    "            s =  r'%s{\\times}%s' % (significand, exponent)\n",
    "        else:\n",
    "            s =  r'%s%s' % (significand, exponent)\n",
    "        return \"${}$\".format(s)\n",
    "\n",
    "# Format with 2 decimal places\n",
    "plt.gca().yaxis.set_major_formatter(MathTextSciFormatter(\"%1.2e\"))\n",
    "# Plot Top Languages\n",
    "plt.title(\"Number of Samples for Top Languages\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=15)\n",
    "top_languages.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages_cleaned = pd.read_pickle(r'temp/post/preprocessed/top_1000_page_preprocessed.plk')\n",
    "# Get the post_id whose post contains emojis\n",
    "filt = top_1000_pages_cleaned['post_message'].apply(lambda x: len(demoji.findall(x)) != 0)\n",
    "emoji_post_ids = top_1000_pages_cleaned.loc[filt, 'post_id']\n",
    "with open(r'temp/temp_variables/emoji_post_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(emoji_post_ids, f) # DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-blind",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "* Reaction\n",
    "    * Reaction on 1000-page\n",
    "    * Reactions on Politicians\n",
    "* Side Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-speaker",
   "metadata": {},
   "source": [
    "# Reaction\n",
    "The reaction data wasn't used in further analysis. Thus, its preprocessing is just for reference as follows.\n",
    "\n",
    "### 1. Reaction on 1000-page: \n",
    "#### A. Every-20-minutes (2016-09-29 ~ 2016-11-21)\n",
    "1. Sum the number of each REACTIONs on Pages for each post in each 20min, and Dump to pickle files (split by chunks)\n",
    " * chunksize = 10000000 rows in the original data (before aggregation)\n",
    "2. Summarize the number of each reactions, and save the summary in a dictionary, `num_reactions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaction (LIKE, LOVE, HAHA, WOW, SAD, ANGRY, THANKFUL)\n",
    "## 1. Reactions on 1000-page\n",
    "#### A. Every-20-minutes (2016-09-29 ~ 2016-11-21)\n",
    "def reaction_pickler(reaction_type, chunksize=10000000):\n",
    "    \"Read in reaction data by chunks, and process them to return desired output.\"\n",
    "    # Create an iterator (chunk_reader) to read files by chunks\n",
    "    path = r'input/reaction/1000-page/20-min/by-reaction-type/{}.csv'.format(reaction_type)\n",
    "    date_parser = lambda unixTime: pd.to_datetime(unixTime, unit='s')\n",
    "    chunk_reader = pd.read_csv(path, usecols=['post_id', 'reaction_time'], parse_dates=['reaction_time'], date_parser=date_parser, chunksize=chunksize)\n",
    "\n",
    "    # Iteratively write \"processed\" chunks to pickle files\n",
    "    for i, chunk in enumerate(chunk_reader):\n",
    "        aggregated_chunk = chunk.groupby(['post_id', 'reaction_time'])['reaction_time'].count()\n",
    "        aggregated_chunk.to_pickle(r'temp/reaction/1000-page/20-min/{}_{}.pkl'.format(reaction_type, i))\n",
    "\n",
    "# Actually pickle the files\n",
    "reaction_types = ['LIKE', 'LOVE', 'HAHA', 'WOW', 'SAD', 'ANGRY', 'THANKFUL']\n",
    "for rt in reaction_types:\n",
    "    reaction_pickler(rt) # DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_types = ['LIKE', 'LOVE', 'HAHA', 'WOW', 'SAD', 'ANGRY', 'THANKFUL']\n",
    "num_reactions = dict(zip(reaction_types, [0]*7))\n",
    "# Store the count for each reactions\n",
    "for f_name in os.listdir(r'temp/reaction/1000-page/20-min'):\n",
    "    reaction = f_name.split('_')[0]\n",
    "    num_reaction = pd.read_pickle(r'temp/reaction/1000-page/20-min/{}'.format(f_name)).sum() # Sum of the Series\n",
    "    try:\n",
    "        num_reactions[reaction] += num_reaction\n",
    "    except KeyError:\n",
    "        print(f\"Reaction {reaction} doesn't exist.\")\n",
    "# Save the variable: num_reactions\n",
    "with open('temp/temp_variables/num_reactions.pkl', 'wb') as f:\n",
    "    pickle.dump(num_reactions, f) # DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thrown-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp/temp_variables/num_reactions.pkl', 'rb') as f:\n",
    "    num_reactions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-apartment",
   "metadata": {},
   "source": [
    "#### B. LIKE by US political users (2015-01-01 ~ 2016-11-29)\n",
    "Sum the number of each REACTIONs on Pages for each post, and Dump to pickle files (split by dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Reactions on 1000-page\n",
    "#### B. LIKE by US political users (2015-01-01 ~ 2016-11-30)\n",
    "like_on_page_post_dates = []\n",
    "src_path = r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date'\n",
    "dest_path = r'temp/reaction/1000-page/LIKE'\n",
    "for fileName in sorted(os.listdir(src_path)):\n",
    "    if not fileName.startswith('.'): # Not a hidden file\n",
    "        like_on_page_post_dates.append(fileName.split('.')[0])\n",
    "\n",
    "for post_date in like_on_page_post_dates:\n",
    "    # Set source and destination files\n",
    "    src_file = os.path.join(src_path, f'{post_date}.csv')\n",
    "    dest_file = os.path.join(dest_path, f'page_likes_{post_date}.pkl')\n",
    "    # Read in a file\n",
    "    temp_df = pd.read_csv(src_file)\n",
    "    # Compute num of likes for each post\n",
    "    aggregated_temp = temp_df.groupby('post_id').count()\n",
    "    # Save as pickle (under the temp directory)\n",
    "    aggregated_temp.to_pickle(dest_file) # DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-senegal",
   "metadata": {},
   "source": [
    "### 2. Reactions on politicians\n",
    "#### LIKE by US political users (2015-05-01 ~ 2016-11-29)\n",
    "Sum the number of LIKEs on Politicians for each post, and Dump to pickle files (split by dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Reactions on Politician \n",
    "#### A. LIKE by US political users (2015-05-01 ~ 2016-11-29) # NOT 11-30!\n",
    "like_on_politician_post_dates = []\n",
    "src_path = r'input/reaction/politician/2015-05-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date'\n",
    "dest_path = r'temp/reaction/politician/LIKE'\n",
    "for fileName in sorted(os.listdir(src_path)):\n",
    "    if not fileName.startswith('.'): # Not a hidden file\n",
    "        like_on_politician_post_dates.append(fileName.split('.')[0])\n",
    "\n",
    "for post_date in like_on_politician_post_dates:\n",
    "    # Set source and destination files\n",
    "    src_file = os.path.join(src_path, f'{post_date}.csv')\n",
    "    dest_file = os.path.join(dest_path, f'politician_likes_{post_date}.pkl')\n",
    "    # Read in a file\n",
    "    temp_df = pd.read_csv(src_file)\n",
    "    # Compute num of likes for each post\n",
    "    aggregated_temp = temp_df.groupby('post_id').count()\n",
    "    # Save as pickle (under the temp directory)\n",
    "    aggregated_temp.to_pickle(dest_file) # DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-taste",
   "metadata": {},
   "source": [
    "### Side Notes:\n",
    "Below is the original intended way to read the files.<br>\n",
    "Unfortunately, the files were too large, and I have to shrink their sizes before actually reading to memory...<br>\n",
    "Thus I need this notebook \"Preprocessing.ipynb\" to pickle everything.\n",
    "```python\n",
    "# Reaction (LIKE, LOVE, HAHA, WOW, SAD, ANGRY, THANKFUL)\n",
    "## 1. Reactions on 1000-page\n",
    "#### A. Every-20-minutes (2016-09-29 ~ 2016-11-21)\n",
    "date_parser = lambda unixTime: pd.to_datetime(unixTime, unit='s')\n",
    "every20min_like = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/LIKE.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_love = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/LOVE.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_haha = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/HAHA.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_wow = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/WOW.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_sad = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/SAD.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_angry = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/ANGRY.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_thankful = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/THANKFUL.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "\n",
    "#### B. LIKE by US political users (2015-01-01 ~ 2016-11-30)\n",
    "# pd.read_csv(r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date/')\n",
    "like_by_post_dates = []\n",
    "for fileName in sorted(os.listdir(r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date')):\n",
    "    if not fileName.startswith('.'): # Not a hidden file\n",
    "        like_by_post_dates.append(fileName.split('.')[0])\n",
    "\n",
    "LIKE_dfs = []\n",
    "for post_date in like_by_post_dates:\n",
    "    path = r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date/{}.csv'.format(post_date)\n",
    "    temp_df = pd.read_csv(path)\n",
    "    temp_df['post_date'] = datetime.strptime(post_date, \"%Y-%m-%d\") # Add a column specifying the date, which is (part of) the fileName\n",
    "    LIKE_dfs.append(temp_df)\n",
    "    \n",
    "LIKE_on_1000_page = pd.concat(LIKE_dfs)\n",
    "\n",
    "## 2. Reactions on Politician \n",
    "#### A. LIKE by US political users (2015-05-01 ~ 2016-11-30)\n",
    "like_by_post_dates = []\n",
    "for fileName in sorted(os.listdir(r'input/reaction/politician/2015-05-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date')):\n",
    "    if not fileName.startswith('.'): # Not a hidden file\n",
    "        like_by_post_dates.append(fileName.split('.')[0])\n",
    "\n",
    "LIKE_dfs = []\n",
    "for post_date in like_by_post_dates:\n",
    "    path = r'input/reaction/politician/2015-05-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date/{}.csv'.format(post_date)\n",
    "    temp_df = pd.read_csv(path)\n",
    "    temp_df['post_date'] = datetime.strptime(post_date, \"%Y-%m-%d\") # Add a column specifying the date, which is (part of) the fileName\n",
    "    LIKE_dfs.append(temp_df)\n",
    "    \n",
    "LIKE_on_politician = pd.concat(LIKE_dfs)\n",
    "\n",
    "# Comment\n",
    "## a total of 500 csv files\n",
    "## NOTE: The files are LARGE! eg. 000000000000.csv, as DataFrame, has a shape of (2863013, 4) and memory usage of 87.4+ MB\n",
    "tables = []\n",
    "for fileName in os.listdir(r'input/comment/2015-01-01-to-2016-11-30'):\n",
    "    tables.append(pd.read_csv(r'input/comment/2015-01-01-to-2016-11-30/{}'.format(fileName), parse_dates=[\"comment_created_time\"]))\n",
    "comments = pd.concat(tables)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
