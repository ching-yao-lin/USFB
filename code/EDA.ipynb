{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worse-means",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-merit",
   "metadata": {},
   "source": [
    "## Reading Files\n",
    "* page_politician_info\n",
    "* top_1000_page\n",
    "* politician_pages\n",
    "\n",
    "## Data Cleaning\n",
    "* top_1000_page\n",
    "\n",
    "## Merging Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "different-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standing-olive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home3/r09725056/Desktop/analysis-ChingYaoL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home3/r09725056/Desktop/analysis-ChingYaoL')\n",
    "print(os.getcwd())\n",
    "# Use relative paths in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brave-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input code output temp .git README.md README_about_USFB_Data.pdf "
     ]
    }
   ],
   "source": [
    "for file in os.listdir():\n",
    "    print(file, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-thomas",
   "metadata": {},
   "source": [
    "#### Suggested workflow from README\n",
    "* Use relative paths in the code\n",
    "* Read data from _input_\n",
    "* Export generated tables or figures to _output_\n",
    "* Read/Write other temporary files from _temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instant-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # 讓 DataFrame Output可以重複疊起來\n",
    "pd.set_option('display.max_columns', None) # show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "about-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automatic-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions \n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "import contractions # resolving contractions and slangs, e.g. \"yall're happy now\" --> \"you all are happy now\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-atlanta",
   "metadata": {},
   "source": [
    "* [PyPI for fasttext](https://pypi.org/project/fasttext/)\n",
    "* [medium for fasttext](https://medium.com/@c.chaitanya/language-identification-in-python-using-fasttext-60359dc30ed0)\n",
    "* [GitHub for contractions](https://github.com/kootenpv/contractions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "incoming-begin",
   "metadata": {},
   "source": [
    "├── page\n",
    "│   ├── 1000-page-info.csv\n",
    "│   ├── politician-info.csv\n",
    "│   └── 1000-page-and-politician-info.csv\n",
    "├── post\n",
    "│   ├── 1000-page\n",
    "│   │   └── 2015-01-01-to-2017-04-08.csv\n",
    "│   └── politician\n",
    "│       └── 2015-01-01-to-2016-11-30.csv\n",
    "├── reaction\n",
    "│   ├── 1000-page\n",
    "│   │   ├── 2015-01-01-to-2016-11-30\n",
    "│   │   │   └── us-political-user\n",
    "│   │   │       └── by-reaction-type\n",
    "│   │   │           └── LIKE\n",
    "│   │   │               └── by-post-date\n",
    "│   │   │                   ├── 2015-01-01.csv\n",
    "│   │   │                   ├── ...(omitted)...\n",
    "│   │   │                   └── 2016-11-30.csv\n",
    "│   │   └── 20-min\n",
    "│   │       └── by-reaction-type\n",
    "│   │           ├── ANGRY.csv\n",
    "│   │           ├── ...(omitted)...\n",
    "│   │           ├── WOW.csv\n",
    "│   │           └── LIKE\n",
    "│   │               └── by-page-id\n",
    "│   │               │   ├── 10018702564.csv\n",
    "│   │               │   ├── ...(omitted)...\n",
    "│   │               │   └── 99881661864.csv\n",
    "│   │               └── by-time-stamp\n",
    "│   │                   ├── 1475161200.csv\n",
    "│   │                   ├── ...(omitted)...\n",
    "│   │                   └── 1479740400.csv\n",
    "│   └── politician\n",
    "│       └── 2015-05-01-to-2016-11-30\n",
    "│           └── us-political-user\n",
    "│               └── by-reaction-type\n",
    "│                   └── LIKE\n",
    "│                       └── by-post-date\n",
    "│                           ├── 2015-05-01.csv\n",
    "│                           ├── ...(omitted)...\n",
    "│                           └── 2016-11-29.csv\n",
    "├── comment\n",
    "│   └── 2015-01-01-to-2016-11-30\n",
    "└── tree.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-arcade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "saving-melbourne",
   "metadata": {},
   "source": [
    "## 2016 US Presidential Election: Nov 8, 2016 (Tue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page\n",
    "page_info = pd.read_csv(r'input/page/1000-page-info.csv')\n",
    "politician_info = pd.read_csv(r'input/page/politician-info.csv')\n",
    "page_politician_info = pd.read_csv(r'input/page/1000-page-and-politician-info.csv')\n",
    "\n",
    "# Post\n",
    "top_1000_pages = pd.read_csv(r'input/post/1000-page/2015-01-01-to-2017-04-08.csv', parse_dates=['post_created_time_CT', 'post_updated_time_CT'])\n",
    "politician_pages = pd.read_csv(r'input/post/politician/2015-01-01-to-2016-11-30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaction (LIKE, LOVE, HAHA, WOW, SAD, ANGRY, THANKFUL)PresidentialPresidential\n",
    "## 1. Reactions on 1000-page\n",
    "#### A. Every-20-minutes (2016-09-29 ~ 2016-11-21)\n",
    "date_parser = lambda unixTime: pd.to_datetime(unixTime, unit='s')\n",
    "every20min_like = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/LIKE.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_love = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/LOVE.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_haha = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/HAHA.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_wow = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/WOW.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_sad = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/SAD.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_angry = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/ANGRY.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "every20min_thankful = pd.read_csv(r'input/reaction/1000-page/20-min/by-reaction-type/THANKFUL.csv', parse_dates=['reaction_time'], date_parser=date_parser)\n",
    "\n",
    "#### B. LIKE by US political users (2015-01-01 ~ 2016-11-30)\n",
    "# pd.read_csv(r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date/')\n",
    "like_by_post_dates = []\n",
    "for fileName in sorted(os.listdir(r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date')):\n",
    "    if not fileName.startswith('.'): # Not a hidden file\n",
    "        like_by_post_dates.append(fileName.split('.')[0])\n",
    "\n",
    "LIKE_dfs = []\n",
    "for post_date in like_by_post_dates:\n",
    "    path = r'input/reaction/1000-page/2015-01-01-to-2016-11-30/us-political-user/by-reaction-type/LIKE/by-post-date/{}.csv'.format(post_date)\n",
    "    temp_df = pd.read_csv(path)\n",
    "    temp_df['post_date'] = datetime.strptime(post_date, \"%Y-%m-%d\") # Add a column specifying the date, which is (part of) the fileName\n",
    "    LIKE_dfs.append(temp_df)\n",
    "    \n",
    "LIKE_on_1000_page = pd.concat(LIKE_dfs)\n",
    "\n",
    "## 2. Reactions on Politician \n",
    "#### A. LIKE by US political users (2015-01-01 ~ 2016-11-30)\n",
    "\n",
    "\n",
    "# Comment\n",
    "## a total of 500 csv files\n",
    "## NOTE: The files are LARGE! eg. 000000000000.csv, as DataFrame, has a shape of (2863013, 4) and memory usage of 87.4+ MB\n",
    "tables = []\n",
    "for fileName in os.listdir(r'input/comment/2015-01-01-to-2016-11-30'):\n",
    "    tables.append(pd.read_csv(r'input/comment/2015-01-01-to-2016-11-30/{}'.format(fileName), parse_dates=[\"comment_created_time\"]))\n",
    "comments = pd.concat(tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info.shape\n",
    "politician_info.shape\n",
    "page_politician_info.shape\n",
    "top_1000_pages.shape\n",
    "politician_pages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of unwanted columns\n",
    "cols_to_drop = ['post_picture', 'post_link', 'post_created_time', 'post_updated_time']\n",
    "top_1000_pages.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_to_nonstring(func):\n",
    "    \"\"\"\n",
    "    A decorator that allows a function to bypass nonstring arguments. That is, func applies only to nonstrings.\n",
    "    \"\"\"\n",
    "    def wrapper_func(x):\n",
    "        try:\n",
    "            return func(x)\n",
    "        except IndexError:\n",
    "            return x\n",
    "        except AttributeError:\n",
    "            return x\n",
    "    return wrapper_func\n",
    "\n",
    "@func_to_nonstring\n",
    "def contract_message(x):\n",
    "    return contractions.fix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['post_message'] = top_1000_pages['post_message'].apply(contract_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_1000_pages['post_message'].isna().sum())\n",
    "print(top_1000_pages['post_message'].notna().sum())\n",
    "print(f\"null rate: {top_1000_pages['post_message'].isna().sum() / top_1000_pages['post_message'].notna().sum():.6f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-replication",
   "metadata": {},
   "source": [
    "### English Lanuage Detection\n",
    "The reason I use fasttext: [Benchmarking Language Detection for NLP](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"/home3/r09725056/.conda/envs/usfb/lib/python3.7/site-packages/fasttext/lid.176.bin\"\n",
    "model = fasttext.load_model(pretrained_model)\n",
    "\n",
    "@func_to_nonstring\n",
    "def predict_language(sent):\n",
    "    sent = sent.replace('\\n', ' ')\n",
    "    pred = model.predict(sent) # model.predict() returns a tuple like this: (('__label__en',), array([0.95346403]))\n",
    "    return pred[0][0].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['language'] = top_1000_pages['post_message'].apply(predict_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['language'].value_counts(dropna=False, normalize=True).round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_unique_elements(ser, thres=0.001, ndecimal=6, show_null_rate=True):\n",
    "    \"\"\"\n",
    "    Return top unique elements which take up no less than 0.001 (or thres) in the entire pd.Series\n",
    "    This is done through ser.value_counts(normalize=True, dropna=False)\n",
    "    \"\"\"\n",
    "    filt = (ser.value_counts(dropna=False, normalize=True) >= thres)\n",
    "    if show_null_rate:\n",
    "        print(f\"null rate = {ser.isnull().mean() * 100:.6f} %\")\n",
    "    return ser.value_counts(dropna=False).loc[filt].round(ndecimal)\n",
    "\n",
    "# filter for languages that's at least 0.1%, then assign to top_languages\n",
    "lang_thres = 0.001\n",
    "top_languages = filter_top_unique_elements(top_1000_pages['language'], lang_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler way to format yticks\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# f = mticker.ScalarFormatter(useOffset=False, useMathText=True)\n",
    "# g = lambda x, pos : \"${}$\".format(f._formatSciNotation('%1.10e' % x))\n",
    "# plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(g))\n",
    "# top_languages.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Customize Yticks\n",
    "class MathTextSciFormatter(mticker.Formatter):\n",
    "    def __init__(self, fmt=\"%1.2e\"):\n",
    "        self.fmt = fmt\n",
    "    def __call__(self, x, pos=None):\n",
    "        s = self.fmt % x\n",
    "        decimal_point = '.'\n",
    "        positive_sign = '+'\n",
    "        tup = s.split('e')\n",
    "        significand = tup[0].rstrip(decimal_point)\n",
    "        sign = tup[1][0].replace(positive_sign, '')\n",
    "        exponent = tup[1][1:].lstrip('0')\n",
    "        if exponent:\n",
    "            exponent = '10^{%s%s}' % (sign, exponent)\n",
    "        if significand and exponent:\n",
    "            s =  r'%s{\\times}%s' % (significand, exponent)\n",
    "        else:\n",
    "            s =  r'%s%s' % (significand, exponent)\n",
    "        return \"${}$\".format(s)\n",
    "\n",
    "# Format with 2 decimal places\n",
    "plt.gca().yaxis.set_major_formatter(MathTextSciFormatter(\"%1.2e\"))\n",
    "# Plot Top Languages\n",
    "plt.title(\"Number of Samples for Top Languages\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=15)\n",
    "top_languages.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for languages that's at least 0.01%\n",
    "filter_top_unique_elements(top_1000_pages['post_name'], thres=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['language'].value_counts(normalize=True, dropna=False).apply(lambda x: f\"{x * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages['post_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_unique_elements(top_1000_pages['post_caption'], thres=0)\n",
    "# It seems that captions are usually webpages or links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_top_unique_elements(top_1000_pages['post_description'], thres=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-clearance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-showcase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-reggae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-arcade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-columbus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-xerox",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-square",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-progressive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-stylus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
